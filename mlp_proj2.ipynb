{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc214e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.2\n",
      "PyTorch: 2.9.0\n",
      "MPS available: True\n",
      "Using device: mps\n",
      "Matmul ok: torch.Size([1000, 1000])\n",
      "Matmul ok: torch.Size([1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "# MPS (Apple Silicon) check and default device setup\n",
    "import torch, platform\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "mps_ok = torch.backends.mps.is_available() and torch.backends.mps.is_built()\n",
    "device = torch.device(\"mps\") if mps_ok else torch.device(\"cpu\")\n",
    "print(f\"MPS available: {mps_ok}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# quick sanity test on selected device\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "y = torch.mm(x, x.T)\n",
    "print(\"Matmul ok:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26045052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q install pandas numpy scikit-learn lightgbm sentence-transformers transformers peft accelerate datasets evaluate\n",
    "import pandas as pd, numpy as np, os, gc, math, random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, confusion_matrix, classification_report\n",
    "RNG = 42\n",
    "random.seed(RNG); np.random.seed(RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee1d085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (57477, 9)\n",
      "Test shape: (3, 4)\n",
      "Train columns: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
      "Test columns: ['id', 'prompt', 'response_a', 'response_b']\n",
      "Sample submission columns: ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
      "ID column: id\n",
      "Using soft targets (converted to hard labels via argmax).\n",
      "Num classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Paths and data loading\n",
    "import pandas as pd, numpy as np, os, json, math, re, string\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "BASE = Path('llm-classification-finetuning')\n",
    "TRAIN_PATH = BASE / 'train.csv'\n",
    "TEST_PATH = BASE / 'test.csv'\n",
    "SAMPLE_SUB_PATH = BASE / 'sample_submission.csv'\n",
    "\n",
    "assert TRAIN_PATH.exists(), f\"Missing {TRAIN_PATH}\"\n",
    "assert TEST_PATH.exists(), f\"Missing {TEST_PATH}\"\n",
    "assert SAMPLE_SUB_PATH.exists(), f\"Missing {SAMPLE_SUB_PATH}\"\n",
    "\n",
    "# Read CSVs (train is large, so let pandas stream types)\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "\n",
    "print('Train shape:', train.shape)\n",
    "print('Test shape:', test.shape)\n",
    "print('Train columns:', list(train.columns))\n",
    "print('Test columns:', list(test.columns))\n",
    "print('Sample submission columns:', list(sample_sub.columns))\n",
    "\n",
    "# Expected text columns\n",
    "TEXT_COLS = ['prompt', 'response_a', 'response_b']\n",
    "for c in TEXT_COLS:\n",
    "    if c not in train.columns:\n",
    "        raise ValueError(f\"Expected column '{c}' in train.csv\")\n",
    "for c in TEXT_COLS:\n",
    "    if c not in test.columns:\n",
    "        raise ValueError(f\"Expected column '{c}' in test.csv\")\n",
    "ID_COL = 'id' if 'id' in test.columns else test.columns[0]\n",
    "print('ID column:', ID_COL)\n",
    "\n",
    "# Determine targets\n",
    "SOFT_TARGETS = ['winner_model_a','winner_model_b','winner_tie']\n",
    "HARD_TARGET = None\n",
    "if all(c in train.columns for c in SOFT_TARGETS):\n",
    "    y_soft = train[SOFT_TARGETS].values.astype(float)\n",
    "    y = y_soft.argmax(axis=1)  # 0=A, 1=B, 2=Tie\n",
    "    print('Using soft targets (converted to hard labels via argmax).')\n",
    "elif 'winner' in train.columns:\n",
    "    # Map common strings to class ids\n",
    "    mapping = {\n",
    "        'model_a': 0, 'a': 0, 'A': 0, 0: 0,\n",
    "        'model_b': 1, 'b': 1, 'B': 1, 1: 1,\n",
    "        'tie': 2, 'TIE': 2, 2: 2,\n",
    "    }\n",
    "    y = train['winner'].map(mapping)\n",
    "    if y.isna().any():\n",
    "        raise ValueError('Unknown labels in winner column; please adjust mapping.')\n",
    "    y = y.astype(int).values\n",
    "    y_soft = None\n",
    "    print('Using hard labels from winner column.')\n",
    "elif 'label' in train.columns:\n",
    "    # Assume values {0,1,2} -> A,B,Tie\n",
    "    y = train['label'].astype(int).values\n",
    "    y_soft = None\n",
    "    print('Using hard integer labels from label column.')\n",
    "else:\n",
    "    raise ValueError('Could not find target columns. Expected winner_model_* or winner/label.')\n",
    "num_classes = 3\n",
    "print('Num classes:', num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline feature engineering: lexical/length features + Logistic Regression\n",
    "import numpy as np, pandas as pd, re, string, math, gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, classification_report\n",
    "\n",
    "puncts = set(string.punctuation)\n",
    "\n",
    "def safe_len(s: str) -> int:\n",
    "    return len(s) if isinstance(s, str) else 0\n",
    "\n",
    "def words(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    return re.findall(r\"\\b\\w+\\b\", s.lower())\n",
    "\n",
    "def text_feats(s: str) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\"\n",
    "    w = words(s)\n",
    "    wc = len(w)\n",
    "    char_len = len(s)\n",
    "    avg_wlen = (sum(len(x) for x in w) / wc) if wc else 0.0\n",
    "    num_digits = sum(ch.isdigit() for ch in s)\n",
    "    num_upper = sum(ch.isupper() for ch in s)\n",
    "    num_punct = sum(ch in puncts for ch in s)\n",
    "    excl = s.count('!')\n",
    "    ques = s.count('?')\n",
    "    commas = s.count(',')\n",
    "    periods = s.count('.')\n",
    "    newlines = s.count('\\n')\n",
    "    cap_ratio = (num_upper / char_len) if char_len else 0.0\n",
    "    digit_ratio = (num_digits / char_len) if char_len else 0.0\n",
    "    punct_ratio = (num_punct / max(char_len,1))\n",
    "    return {\n",
    "        'char_len': char_len,\n",
    "        'word_count': wc,\n",
    "        'avg_word_len': avg_wlen,\n",
    "        'num_digits': num_digits,\n",
    "        'num_upper': num_upper,\n",
    "        'num_punct': num_punct,\n",
    "        'excl': excl,\n",
    "        'ques': ques,\n",
    "        'commas': commas,\n",
    "        'periods': periods,\n",
    "        'newlines': newlines,\n",
    "        'cap_ratio': cap_ratio,\n",
    "        'digit_ratio': digit_ratio,\n",
    "        'punct_ratio': punct_ratio,\n",
    "    }\n",
    "\n",
    "def build_pair_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Compute features for prompt and both responses\n",
    "    p_feats = df['prompt'].map(text_feats).apply(pd.Series).add_prefix('p_')\n",
    "    a_feats = df['response_a'].map(text_feats).apply(pd.Series).add_prefix('a_')\n",
    "    b_feats = df['response_b'].map(text_feats).apply(pd.Series).add_prefix('b_')\n",
    "    # Pairwise differences and ratios (verbosity/bias-aware)\n",
    "    diff = (a_feats - b_feats).add_prefix('diff_')\n",
    "    ad = (a_feats - b_feats).abs().add_prefix('abs_')\n",
    "    # Ratios (add small constant)\n",
    "    eps = 1e-6\n",
    "    ratio_cols = {}\n",
    "    for col in [c for c in a_feats.columns if c.startswith('a_')]:\n",
    "        base = col[2:]\n",
    "        ac = a_feats[col].astype(float)\n",
    "        bc = b_feats['b_'+base].astype(float)\n",
    "        ratio = (ac + eps) / (bc + eps)\n",
    "        ratio = ratio.replace([np.inf, -np.inf], np.nan).fillna(1.0)\n",
    "        ratio_cols[f'ratio_{base}'] = ratio\n",
    "    ratio = pd.DataFrame(ratio_cols, index=df.index)\n",
    "    # Position bias proxy: longer response tends to be preferred (captured above); also include which is longer\n",
    "    longer_a = (a_feats['a_char_len'] > b_feats['b_char_len']).astype(int).rename('a_is_longer')\n",
    "    # Combine\n",
    "    X = pd.concat([p_feats, a_feats, b_feats, diff, ad, ratio, longer_a], axis=1)\n",
    "    # Clean NaNs/Infs\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return X\n",
    "\n",
    "# Build features\n",
    "X_baseline = build_pair_features(train)\n",
    "X_test_baseline = build_pair_features(test)\n",
    "print('Baseline feature matrix shapes:', X_baseline.shape, X_test_baseline.shape)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "X_all = scaler.fit_transform(X_baseline.values)\n",
    "X_tst = scaler.transform(X_test_baseline.values)\n",
    "\n",
    "# CV training\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
    "oof = np.zeros((len(train), 3), dtype=float)\n",
    "test_pred = np.zeros((len(test), 3), dtype=float)\n",
    "fold_reports = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X_all, y), 1):\n",
    "    X_tr, X_va = X_all[tr_idx], X_all[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, C=2.0, class_weight=None, random_state=RNG)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    proba_va = clf.predict_proba(X_va)\n",
    "    oof[va_idx] = proba_va\n",
    "    ll = log_loss(y_true=y_va, y_pred=proba_va)\n",
    "    fold_reports.append((fold, ll))\n",
    "    test_pred += clf.predict_proba(X_tst) / skf.n_splits\n",
    "    print(f\"Fold {fold} log_loss: {ll:.5f}\")\n",
    "    gc.collect()\n",
    "\n",
    "oof_ll = log_loss(y_true=y, y_pred=oof)\n",
    "print(f\"OOF log_loss: {oof_ll:.5f}\")\n",
    "\n",
    "# Build submission from baseline\n",
    "sub_baseline = pd.DataFrame({\n",
    "    ID_COL: test[ID_COL].values,\n",
    "    'winner_model_a': test_pred[:,0],\n",
    "    'winner_model_b': test_pred[:,1],\n",
    "    'winner_tie': test_pred[:,2],\n",
    "})\n",
    "sub_baseline.to_csv('submission_baseline.csv', index=False)\n",
    "print('Wrote submission_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa5891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Submit to Kaggle (set COMPETITION and ensure Kaggle API is configured)\n",
    "import os, subprocess, sys\n",
    "COMPETITION = os.environ.get('KAGGLE_COMPETITION', 'REPLACE_WITH_COMPETITION_SLUG')  # e.g., 'llm-classification-finetuning'\n",
    "SUB_FILE = 'submission_baseline.csv'\n",
    "MESSAGE = 'baseline lexical+LR'\n",
    "\n",
    "def try_kaggle_submit(competition: str, file_path: str, message: str):\n",
    "    try:\n",
    "        import kaggle  # noqa: F401\n",
    "    except Exception as e:\n",
    "        print('Kaggle package not found; install with `pip install kaggle` and set credentials.', e)\n",
    "        return\n",
    "    if competition.startswith('REPLACE_') or competition.startswith('replace_') or competition=='REPLACE_WITH_COMPETITION_SLUG':\n",
    "        print('Set COMPETITION to the correct Kaggle competition slug before submitting.')\n",
    "        return\n",
    "    if not os.path.exists(file_path):\n",
    "        print('Submission file not found:', file_path)\n",
    "        return\n",
    "    # Run Kaggle CLI\n",
    "    cmd = ['kaggle','competitions','submit','-c', competition, '-f', file_path, '-m', message]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    try:\n",
    "        out = subprocess.run(cmd, capture_output=True, text=True, check=False)\n",
    "        print('Return code:', out.returncode)\n",
    "        print('STDOUT:\\n', out.stdout)\n",
    "        print('STDERR:\\n', out.stderr)\n",
    "    except Exception as e:\n",
    "        print('Submission failed with exception:', e)\n",
    "\n",
    "# try_kaggle_submit(COMPETITION, SUB_FILE, MESSAGE)  # Uncomment to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding-based model: Sentence-Transformers (MiniLM) + Logistic Regression\n",
    "from sentence_transformers import SentenceTransformer, util as st_util\n",
    "import numpy as np, pandas as pd, gc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "embed_model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "st_device = str(device) if 'device' in globals() else 'cpu'\n",
    "print('Loading embedding model:', embed_model_name, 'on', st_device)\n",
    "st_model = SentenceTransformer(embed_model_name, device=st_device)\n",
    "\n",
    "def encode_texts(texts, batch_size=256, show_progress_bar=True):\n",
    "    return st_model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=show_progress_bar, normalize_embeddings=True)\n",
    "\n",
    "def build_embedding_features(df: pd.DataFrame) -> np.ndarray:\n",
    "    prompts = df['prompt'].fillna('').tolist()\n",
    "    respa = df['response_a'].fillna('').tolist()\n",
    "    respb = df['response_b'].fillna('').tolist()\n",
    "    p_emb = encode_texts(prompts)  # [N, D]\n",
    "    a_emb = encode_texts(respa)    # [N, D]\n",
    "    b_emb = encode_texts(respb)    # [N, D]\n",
    "    # Pairwise similarities\n",
    "    cos_ab = (a_emb * b_emb).sum(axis=1, keepdims=True)  # since normalized embeddings\n",
    "    cos_ap = (a_emb * p_emb).sum(axis=1, keepdims=True)\n",
    "    cos_bp = (b_emb * p_emb).sum(axis=1, keepdims=True)\n",
    "    # Combine features\n",
    "    X = np.concatenate([\n",
    "        a_emb, b_emb, p_emb,\n",
    "        a_emb - b_emb, np.abs(a_emb - b_emb),\n",
    "        cos_ab, cos_ap, cos_bp\n",
    "    ], axis=1)\n",
    "    return X\n",
    "\n",
    "# Build features\n",
    "X_emb_tr = build_embedding_features(train)\n",
    "X_emb_te = build_embedding_features(test)\n",
    "print('Embedding feature shapes:', X_emb_tr.shape, X_emb_te.shape)\n",
    "\n",
    "# Scale + CV train\n",
    "scaler_emb = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr = scaler_emb.fit_transform(X_emb_tr)\n",
    "Xte = scaler_emb.transform(X_emb_te)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RNG)\n",
    "oof_e = np.zeros((len(train), 3), dtype=float)\n",
    "test_pred_e = np.zeros((len(test), 3), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(Xtr, y), 1):\n",
    "    X_tr, X_va = Xtr[tr_idx], Xtr[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "    clf = LogisticRegression(max_iter=2000, solver='saga', n_jobs=-1, C=4.0, class_weight=None, random_state=RNG)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    proba_va = clf.predict_proba(X_va)\n",
    "    oof_e[va_idx] = proba_va\n",
    "    ll = log_loss(y_true=y_va, y_pred=proba_va)\n",
    "    print(f\"[Emb] Fold {fold} log_loss: {ll:.5f}\")\n",
    "    test_pred_e += clf.predict_proba(Xte) / skf.n_splits\n",
    "    gc.collect()\n",
    "\n",
    "oof_ll_e = log_loss(y_true=y, y_pred=oof_e)\n",
    "print(f\"[Emb] OOF log_loss: {oof_ll_e:.5f}\")\n",
    "\n",
    "# Save embedding submission\n",
    "sub_emb = pd.DataFrame({\n",
    "    ID_COL: test[ID_COL].values,\n",
    "    'winner_model_a': test_pred_e[:,0],\n",
    "    'winner_model_b': test_pred_e[:,1],\n",
    "    'winner_tie': test_pred_e[:,2],\n",
    "})\n",
    "sub_emb.to_csv('submission_embedding.csv', index=False)\n",
    "print('Wrote submission_embedding.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea125403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration and simple ensembling\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calibrate_probs(oof_probs: np.ndarray, y_true: np.ndarray, test_probs: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    # Fit a simple multinomial logistic regression on predicted probabilities as features\n",
    "    clf = LogisticRegression(max_iter=2000, solver='lbfgs', C=1.0)\n",
    "    clf.fit(oof_probs, y_true)\n",
    "    cal_oof = clf.predict_proba(oof_probs)\n",
    "    cal_test = clf.predict_proba(test_probs)\n",
    "    ll = log_loss(y_true, cal_oof, labels=[0,1,2])\n",
    "    return cal_test, ll\n",
    "\n",
    "# Calibrate baseline\n",
    "cal_test_base, ll_base_cal = calibrate_probs(oof, y, test_pred)\n",
    "print(f\"Baseline calibrated OOF log_loss (stacking): {ll_base_cal:.5f}\")\n",
    "\n",
    "# Calibrate embedding\n",
    "cal_test_emb, ll_emb_cal = calibrate_probs(oof_e, y, test_pred_e)\n",
    "print(f\"Embedding calibrated OOF log_loss (stacking): {ll_emb_cal:.5f}\")\n",
    "\n",
    "# Ensemble (equal weight)\n",
    "ens_test = 0.5 * cal_test_base + 0.5 * cal_test_emb\n",
    "\n",
    "sub_ens = pd.DataFrame({\n",
    "    ID_COL: test[ID_COL].values,\n",
    "    'winner_model_a': ens_test[:,0],\n",
    "    'winner_model_b': ens_test[:,1],\n",
    "    'winner_tie': ens_test[:,2],\n",
    "})\n",
    "sub_ens.to_csv('submission_ensemble.csv', index=False)\n",
    "print('Wrote submission_ensemble.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Lightweight fine-tuning (DeBERTa-small + LoRA)\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np, pandas as pd, torch, os, gc\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "RUN_LORA = False  # Set True to run a tiny LoRA finetune (can be slow on CPU)\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'\n",
    "NUM_LABELS = 3\n",
    "MAX_LEN = 512\n",
    "BATCH = 8\n",
    "EPOCHS = 1\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, y: Optional[np.ndarray], tok: AutoTokenizer, max_len: int = 512):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.y = None if y is None else y.astype(int)\n",
    "        self.tok = tok\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = f\"Prompt:\\n{row['prompt']}\\n\\nResponse A:\\n{row['response_a']}\\n\\nResponse B:\\n{row['response_b']}\"\n",
    "        enc = self.tok(text, truncation=True, max_length=self.max_len)\n",
    "        item = {k: torch.tensor(v) for k,v in enc.items()}\n",
    "        if self.y is not None:\n",
    "            item['labels'] = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def run_lora_training(train_df: pd.DataFrame, y: np.ndarray, eval_frac: float = 0.05):\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    # Small eval split\n",
    "    n = len(train_df)\n",
    "    n_eval = max(100, int(n * eval_frac))\n",
    "    eval_idx = np.random.RandomState(RNG).choice(n, size=n_eval, replace=False)\n",
    "    tr_mask = np.ones(n, dtype=bool)\n",
    "    tr_mask[eval_idx] = False\n",
    "    dtrain = PairDataset(train_df[tr_mask], y[tr_mask], tok, MAX_LEN)\n",
    "    deval = PairDataset(train_df[~tr_mask], y[~tr_mask], tok, MAX_LEN)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "    lconf = LoraConfig(task_type='SEQ_CLS', r=8, lora_alpha=16, lora_dropout=0.05)\n",
    "    model = get_peft_model(model, lconf)\n",
    "    args = TrainingArguments(\n",
    "        output_dir='out_lora',\n",
    "        per_device_train_batch_size=BATCH,\n",
    "        per_device_eval_batch_size=BATCH,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=200,\n",
    "        save_strategy='no',\n",
    "        report_to=[]\n",
    "    )\n",
    "    collate = DataCollatorWithPadding(tok)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=dtrain,\n",
    "        eval_dataset=deval,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collate,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return model, tok\n",
    "\n",
    "if RUN_LORA:\n",
    "    model_lora, tok_lora = run_lora_training(train, y)\n",
    "    # Inference on test\n",
    "    class TstDataset(Dataset):\n",
    "        def __init__(self, df, tok):\n",
    "            self.df = df.reset_index(drop=True)\n",
    "            self.tok = tok\n",
    "        def __len__(self):\n",
    "            return len(self.df)\n",
    "        def __getitem__(self, idx):\n",
    "            row = self.df.iloc[idx]\n",
    "            text = f\"Prompt:\\n{row['prompt']}\\n\\nResponse A:\\n{row['response_a']}\\n\\nResponse B:\\n{row['response_b']}\"\n",
    "            return self.tok(text, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "    td = TstDataset(test, tok_lora)\n",
    "    # Simple loop (avoid Trainer.predict to keep it lightweight)\n",
    "    model_lora.eval()\n",
    "    preds = []\n",
    "    for i in range(len(td)):\n",
    "        batch = td[i]\n",
    "        batch = {k: v.squeeze(0).to(model_lora.device) for k,v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model_lora(**batch)\n",
    "            p = torch.softmax(out.logits, dim=-1).cpu().numpy()\n",
    "        preds.append(p)\n",
    "    preds = np.vstack(preds)\n",
    "    sub_lora = pd.DataFrame({\n",
    "        ID_COL: test[ID_COL].values,\n",
    "        'winner_model_a': preds[:,0],\n",
    "        'winner_model_b': preds[:,1],\n",
    "        'winner_tie': preds[:,2],\n",
    "    })\n",
    "    sub_lora.to_csv('submission_lora.csv', index=False)\n",
    "    print('Wrote submission_lora.csv')\n",
    "else:\n",
    "    print('LoRA training skipped. Set RUN_LORA=True to run (may take time).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
