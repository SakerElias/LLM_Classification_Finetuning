{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0addce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offline mode and logging config (Kaggle-friendly)\n",
    "import os\n",
    "\n",
    "# Force offline behavior for Hugging Face\n",
    "os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "\n",
    "# Avoid importing TF/JAX backends via transformers and quiet TF/XLA logs\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "# Use a writable cache dir on Kaggle if available\n",
    "if os.path.isdir(\"/kaggle/working\"):\n",
    "    os.environ.setdefault(\"HF_HOME\", \"/kaggle/working/hfcache\")\n",
    "    os.environ.setdefault(\"HF_HUB_CACHE\", \"/kaggle/working/hfcache\")\n",
    "\n",
    "print(\"HF offline:\", os.environ.get(\"HF_HUB_OFFLINE\"), \"Transformers offline:\", os.environ.get(\"TRANSFORMERS_OFFLINE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23592e9",
   "metadata": {},
   "source": [
    "# Step 3 — Extensions: Bias features, Calibration, Ensembling, and LoRA fine-tuning\n",
    "\n",
    "This notebook implements the full Step 3 pipeline without touching prior notebooks:\n",
    "\n",
    "- Bias-aware lexical features (verbosity and structure)\n",
    "- Calibrated classifiers (sigmoid and isotonic)\n",
    "- Embeddings-based model (reusing precomputed .npy when available)\n",
    "- Simple ensembling via OOF-weight search\n",
    "- Optional lightweight LoRA fine-tuning with temperature scaling\n",
    "\n",
    "Outputs: submission CSVs for each component and a blended ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61179c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:55:54.939360Z",
     "iopub.status.busy": "2025-11-04T18:55:54.939113Z",
     "iopub.status.idle": "2025-11-04T18:55:56.799542Z",
     "shell.execute_reply": "2025-11-04T18:55:56.798940Z",
     "shell.execute_reply.started": "2025-11-04T18:55:54.939345Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "import os, re, ast, json, math, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CUDA / device setup\n",
    "import torch\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "if USE_CUDA:\n",
    "    try:\n",
    "        print('Using CUDA device:', torch.cuda.get_device_name(0))\n",
    "        # Prefer higher matmul precision on GPU; allow TF32 if available (PyTorch 2+ and supported GPUs)\n",
    "        torch.set_float32_matmul_precision('high')\n",
    "        if hasattr(torch.backends, 'cuda') and hasattr(torch.backends.cuda, 'matmul'):\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        if hasattr(torch.backends, 'cudnn'):\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "    except Exception as e:\n",
    "        print('CUDA setup note:', e)\n",
    "else:\n",
    "    print('CUDA not available; using CPU')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "# Kaggle paths (these are read-only under /kaggle/input)\n",
    "TRAIN_PATH = '/kaggle/input/llm-classification-finetuning/train.csv'\n",
    "TEST_PATH = '/kaggle/input/llm-classification-finetuning/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Build 3-class target consistent with previous notebooks: 0=A, 1=B, 2=Tie\n",
    "y = np.select([train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1], [0,1,2])\n",
    "train_df['target'] = y\n",
    "classes = [0,1,2]  # ensure consistent ordering for log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aee06ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:55:56.801277Z",
     "iopub.status.busy": "2025-11-04T18:55:56.800894Z",
     "iopub.status.idle": "2025-11-04T18:56:00.685936Z",
     "shell.execute_reply": "2025-11-04T18:56:00.685327Z",
     "shell.execute_reply.started": "2025-11-04T18:55:56.801253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Text extraction utilities (same behavior as earlier notebook)\n",
    "def extract_text_from_field(text_field):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text_field)\n",
    "        return ' '.join(parsed) if isinstance(parsed, list) else str(parsed)\n",
    "    except Exception:\n",
    "        return str(text_field)\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n",
    "    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n",
    "    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n",
    "    df['text_a'] = df['prompt_text'] + ' [SEP] ' + df['response_a_text']\n",
    "    df['text_b'] = df['prompt_text'] + ' [SEP] ' + df['response_b_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a26d75",
   "metadata": {},
   "source": [
    "## Bias-aware and structural lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57487ed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:56:00.686818Z",
     "iopub.status.busy": "2025-11-04T18:56:00.686604Z",
     "iopub.status.idle": "2025-11-04T18:56:11.132925Z",
     "shell.execute_reply": "2025-11-04T18:56:11.132157Z",
     "shell.execute_reply.started": "2025-11-04T18:56:00.686802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 11), (3, 11))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structural counters\n",
    "\n",
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "\n",
    "def paragraph_count(t):\n",
    "    return t.count('\\n\\n') if isinstance(t, str) else 0\n",
    "\n",
    "\n",
    "def list_count(t):\n",
    "    return count_pattern(t, r'(^\\s*[\\-\\*•]\\s|\\d+\\.)')\n",
    "\n",
    "\n",
    "def quote_count(t):\n",
    "    return count_pattern(t, r'>|\\*\\*')\n",
    "\n",
    "\n",
    "def sentence_count(t):\n",
    "    return count_pattern(t, r'[.!?](\\s|$)')\n",
    "\n",
    "\n",
    "def code_block_count(t):\n",
    "    return count_pattern(t, r'```|`[^`]+`')\n",
    "\n",
    "\n",
    "def heading_count(t):\n",
    "    return count_pattern(t, r'^(#|##|###|####|#####|######)\\s')\n",
    "\n",
    "\n",
    "def word_count(t):\n",
    "    return len(t.split()) if isinstance(t, str) else 0\n",
    "\n",
    "# Compute per-side features using explicit suffix mapping ('a' / 'b')\n",
    "for df in (train_df, test_df):\n",
    "    for col, suffix in [('response_a_text', 'a'), ('response_b_text', 'b')]:\n",
    "        df[f'len_{suffix}']   = df[col].astype(str).apply(len)\n",
    "        df[f'wc_{suffix}']    = df[col].astype(str).apply(word_count)\n",
    "        df[f'sent_{suffix}']  = df[col].apply(sentence_count)\n",
    "        df[f'para_{suffix}']  = df[col].apply(paragraph_count)\n",
    "        df[f'list_{suffix}']  = df[col].apply(list_count)\n",
    "        df[f'quote_{suffix}'] = df[col].apply(quote_count)\n",
    "        df[f'code_{suffix}']  = df[col].apply(code_block_count)\n",
    "        df[f'hdr_{suffix}']   = df[col].apply(heading_count)\n",
    "\n",
    "    # Diffs (A - B) — captures verbosity and structure bias\n",
    "    for base in ['len','wc','sent','para','list','quote','code','hdr']:\n",
    "        df[f'{base}_diff'] = df[f'{base}_a'] - df[f'{base}_b']\n",
    "\n",
    "    # Ratios (A / (B+1)) to capture scale-invariant verbosity bias\n",
    "    for base in ['len','wc','sent']:\n",
    "        df[f'{base}_ratio'] = df[f'{base}_a'] / (df[f'{base}_b'] + 1.0)\n",
    "\n",
    "LEX_FEATURES = [\n",
    "    'len_diff','wc_diff','sent_diff','para_diff','list_diff','quote_diff','code_diff','hdr_diff',\n",
    "    'len_ratio','wc_ratio','sent_ratio'\n",
    "]\n",
    "X_lex = train_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex_test = test_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex.shape, X_lex_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1f8a7",
   "metadata": {},
   "source": [
    "## Calibrated lexical model (sigmoid and isotonic) with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9917e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T18:56:11.134042Z",
     "iopub.status.busy": "2025-11-04T18:56:11.133736Z",
     "iopub.status.idle": "2025-11-04T18:56:23.768408Z",
     "shell.execute_reply": "2025-11-04T18:56:23.767703Z",
     "shell.execute_reply.started": "2025-11-04T18:56:11.134019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lex sigmoid] Fold 1: log_loss=1.06936\n",
      "[Lex sigmoid] Fold 2: log_loss=1.06919\n",
      "[Lex sigmoid] Fold 3: log_loss=1.07031\n",
      "[Lex sigmoid] Fold 4: log_loss=1.07064\n",
      "[Lex sigmoid] Fold 5: log_loss=1.06862\n",
      "[Lex sigmoid] OOF log_loss: 1.06962\n",
      "[Lex isotonic] Fold 1: log_loss=1.06081\n",
      "[Lex isotonic] Fold 2: log_loss=1.06201\n",
      "[Lex isotonic] Fold 3: log_loss=1.06128\n",
      "[Lex isotonic] Fold 4: log_loss=1.05980\n",
      "[Lex isotonic] Fold 5: log_loss=1.05939\n",
      "[Lex isotonic] OOF log_loss: 1.06066\n",
      "Best lexical calibration: isotonic OOF log_loss= 1.060656514215499\n"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs(X, y, base_model=None, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "    if base_model is None:\n",
    "        base_model = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models = []\n",
    "    scalers = []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        clf = CalibratedClassifierCV(estimator=base_model, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Lex {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf)\n",
    "        scalers.append(scaler)\n",
    "    print(f'[Lex {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_lex_sigmoid, lex_sigmoid_models, lex_sigmoid_scalers = cv_calibrated_probs(X_lex, y, method='sigmoid')\n",
    "oof_lex_isotonic, lex_isotonic_models, lex_isotonic_scalers = cv_calibrated_probs(X_lex, y, method='isotonic')\n",
    "\n",
    "# Choose the better calibration by OOF loss\n",
    "lex_oof_list = [('sigmoid', oof_lex_sigmoid), ('isotonic', oof_lex_isotonic)]\n",
    "lex_best_name, lex_best_oof = min(lex_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best lexical calibration:', lex_best_name, 'OOF log_loss=', log_loss(y, lex_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6260c41",
   "metadata": {},
   "source": [
    "## Embedding features (reuse precomputed .npy if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91be0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T19:25:07.364191Z",
     "iopub.status.busy": "2025-11-04T19:25:07.363397Z",
     "iopub.status.idle": "2025-11-04T19:25:07.514406Z",
     "shell.execute_reply": "2025-11-04T19:25:07.513722Z",
     "shell.execute_reply.started": "2025-11-04T19:25:07.364162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 768), (3, 768))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding features (offline-friendly): prefer precomputed .npy; compute only if a local ST model is available\n",
    "import os\n",
    "\n",
    "# Read-only input locations (if a dataset is attached)\n",
    "EMBED_A_TRAIN_IN = '/kaggle/input/embeddings/pytorch/default/1/train_embeddings_a.npy'\n",
    "EMBED_B_TRAIN_IN = '/kaggle/input/embeddings/pytorch/default/1/train_embeddings_b.npy'\n",
    "EMBED_A_TEST_IN  = '/kaggle/input/embeddings/pytorch/default/1/test_embeddings_a.npy'\n",
    "EMBED_B_TEST_IN  = '/kaggle/input/embeddings/pytorch/default/1/test_embeddings_b.npy'\n",
    "\n",
    "# Writable working copies\n",
    "EMBED_A_TRAIN = '/kaggle/working/train_embeddings_a.npy'\n",
    "EMBED_B_TRAIN = '/kaggle/working/train_embeddings_b.npy'\n",
    "EMBED_A_TEST  = '/kaggle/working/test_embeddings_a.npy'\n",
    "EMBED_B_TEST  = '/kaggle/working/test_embeddings_b.npy'\n",
    "\n",
    "# Optional: local folder for Sentence-Transformers model if computing embeddings offline\n",
    "# Attach as dataset and set this to the folder path; default points to a common dataset name\n",
    "ST_EMB_PATH = os.environ.get('ST_EMB_PATH', '/kaggle/input/all-minilm-l6-v2')\n",
    "\n",
    "USE_EMBEDDINGS = False\n",
    "train_a = train_b = test_a = test_b = None\n",
    "\n",
    "\n",
    "def ensure_embeddings(train_df, test_df, batch_size=64, model_path_or_name=ST_EMB_PATH):\n",
    "    \"\"\"Compute embeddings using SentenceTransformer. Works offline only if model_path_or_name is a local folder.\n",
    "    Saves results to /kaggle/working/*.npy\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"sentence-transformers not available in this environment. \"\n",
    "            \"Attach precomputed .npy embeddings or a local model folder to compute.\") from e\n",
    "\n",
    "    device_str = 'cuda' if USE_CUDA else 'cpu'\n",
    "    print(f'Building embeddings on device: {device_str}')\n",
    "    if os.path.isdir(model_path_or_name):\n",
    "        model = SentenceTransformer(model_path_or_name, device=device_str)\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            f\"Sentence-Transformers model not found locally at '{model_path_or_name}'.\\n\"\n",
    "            \"Attach a dataset containing the model folder and set ST_EMB_PATH to that directory.\")\n",
    "\n",
    "    # Use inference mode for speed and memory; convert_to_numpy returns CPU numpy arrays\n",
    "    train_a = model.encode(train_df['text_a'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    train_b = model.encode(train_df['text_b'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_a  = model.encode(test_df['text_a'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_b  = model.encode(test_df['text_b'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    np.save(EMBED_A_TRAIN, train_a); np.save(EMBED_B_TRAIN, train_b)\n",
    "    np.save(EMBED_A_TEST,  test_a);  np.save(EMBED_B_TEST,  test_b)\n",
    "    return train_a, train_b, test_a, test_b\n",
    "\n",
    "# Load order: prefer precomputed input dataset; else working dir; else compute with local model path\n",
    "try:\n",
    "    if all(os.path.exists(p) for p in [EMBED_A_TRAIN_IN, EMBED_B_TRAIN_IN, EMBED_A_TEST_IN, EMBED_B_TEST_IN]):\n",
    "        print('Loading embeddings from input dataset...')\n",
    "        train_a = np.load(EMBED_A_TRAIN_IN)\n",
    "        train_b = np.load(EMBED_B_TRAIN_IN)\n",
    "        test_a  = np.load(EMBED_A_TEST_IN)\n",
    "        test_b  = np.load(EMBED_B_TEST_IN)\n",
    "        USE_EMBEDDINGS = True\n",
    "    elif all(os.path.exists(p) for p in [EMBED_A_TRAIN, EMBED_B_TRAIN, EMBED_A_TEST, EMBED_B_TEST]):\n",
    "        print('Loading embeddings from working dir...')\n",
    "        train_a = np.load(EMBED_A_TRAIN)\n",
    "        train_b = np.load(EMBED_B_TRAIN)\n",
    "        test_a  = np.load(EMBED_A_TEST)\n",
    "        test_b  = np.load(EMBED_B_TEST)\n",
    "        USE_EMBEDDINGS = True\n",
    "    elif os.path.isdir(ST_EMB_PATH):\n",
    "        print('Embeddings not found. Computing now from local Sentence-Transformers model...')\n",
    "        train_a, train_b, test_a, test_b = ensure_embeddings(train_df, test_df)\n",
    "        USE_EMBEDDINGS = True\n",
    "    else:\n",
    "        print('No embeddings available and no local ST model folder found. Proceeding without embeddings.')\n",
    "except Exception as e:\n",
    "    print('Embeddings step failed:', type(e).__name__, '-', str(e))\n",
    "    print('Proceeding without embeddings. You can attach precomputed .npy or set ST_EMB_PATH to a local model folder.')\n",
    "    USE_EMBEDDINGS = False\n",
    "\n",
    "if USE_EMBEDDINGS:\n",
    "    X_emb = np.concatenate([train_a, train_b], axis=1)\n",
    "    X_emb_test = np.concatenate([test_a, test_b], axis=1)\n",
    "    print('Embeddings shapes:', X_emb.shape, X_emb_test.shape)\n",
    "else:\n",
    "    X_emb = None\n",
    "    X_emb_test = None\n",
    "    print('Embeddings disabled for this run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18348",
   "metadata": {},
   "source": [
    "## Calibrated embeddings model with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93cef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T19:25:10.921780Z",
     "iopub.status.busy": "2025-11-04T19:25:10.921449Z",
     "iopub.status.idle": "2025-11-04T19:35:02.234884Z",
     "shell.execute_reply": "2025-11-04T19:35:02.234017Z",
     "shell.execute_reply.started": "2025-11-04T19:25:10.921758Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emb sigmoid] Fold 1: log_loss=1.06656\n",
      "[Emb sigmoid] Fold 2: log_loss=1.06945\n",
      "[Emb sigmoid] Fold 3: log_loss=1.06616\n",
      "[Emb sigmoid] Fold 4: log_loss=1.06705\n",
      "[Emb sigmoid] Fold 5: log_loss=1.06736\n",
      "[Emb sigmoid] OOF log_loss: 1.06732\n",
      "[Emb isotonic] Fold 1: log_loss=1.06543\n",
      "[Emb isotonic] Fold 2: log_loss=1.07141\n",
      "[Emb isotonic] Fold 3: log_loss=1.06569\n",
      "[Emb isotonic] Fold 4: log_loss=1.06628\n",
      "[Emb isotonic] Fold 5: log_loss=1.06670\n",
      "[Emb isotonic] OOF log_loss: 1.06710\n",
      "Best embedding calibration: isotonic OOF log_loss= 1.0671003095562928\n"
     ]
    }
   ],
   "source": [
    "if X_emb is None:\n",
    "    print('Embeddings unavailable; skipping embeddings calibrated models.')\n",
    "    oof_emb_sigmoid = None\n",
    "    oof_emb_isotonic = None\n",
    "    emb_best_oof = None\n",
    "    emb_sigmoid_models = []\n",
    "    emb_isotonic_models = []\n",
    "    emb_sigmoid_scalers = []\n",
    "    emb_isotonic_scalers = []\n",
    "    emb_best_name = None\n",
    "else:\n",
    "    def cv_calibrated_probs_numpy(X, y, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        oof = np.zeros((len(X), 3), dtype=float)\n",
    "        models, scalers = [], []\n",
    "        for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "            Xtr, Xva = X[tr], X[va]\n",
    "            ytr, yva = y[tr], y[va]\n",
    "            scaler = StandardScaler()\n",
    "            Xtr_s = scaler.fit_transform(Xtr)\n",
    "            Xva_s = scaler.transform(Xva)\n",
    "            base = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "            clf = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n",
    "            clf.fit(Xtr_s, ytr)\n",
    "            proba = clf.predict_proba(Xva_s)\n",
    "            oof[va] = proba\n",
    "            loss = log_loss(yva, proba, labels=classes)\n",
    "            print(f'[Emb {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "            models.append(clf); scalers.append(scaler)\n",
    "        print(f'[Emb {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "        return oof, models, scalers\n",
    "\n",
    "    oof_emb_sigmoid, emb_sigmoid_models, emb_sigmoid_scalers = cv_calibrated_probs_numpy(X_emb, y, method='sigmoid')\n",
    "    oof_emb_isotonic, emb_isotonic_models, emb_isotonic_scalers = cv_calibrated_probs_numpy(X_emb, y, method='isotonic')\n",
    "\n",
    "    emb_oof_list = [('sigmoid', oof_emb_sigmoid), ('isotonic', oof_emb_isotonic)]\n",
    "    emb_best_name, emb_best_oof = min(emb_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "    print('Best embedding calibration:', emb_best_name, 'OOF log_loss=', log_loss(y, emb_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c54b32",
   "metadata": {},
   "source": [
    "## Simple ensemble (OOF-weight search on lexical + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63b33c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T19:36:37.842443Z",
     "iopub.status.busy": "2025-11-04T19:36:37.842136Z",
     "iopub.status.idle": "2025-11-04T19:36:38.772010Z",
     "shell.execute_reply": "2025-11-04T19:36:38.771255Z",
     "shell.execute_reply.started": "2025-11-04T19:36:37.842422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble best weight (lexical): 0.59, OOF log_loss: 1.05406\n"
     ]
    }
   ],
   "source": [
    "# Grid search weights w in [0..1] for p = w*p_lex + (1-w)*p_emb minimizing OOF log_loss\n",
    "if 'lex_best_oof' not in globals():\n",
    "    raise RuntimeError('lex_best_oof not computed earlier')\n",
    "\n",
    "if 'emb_best_oof' in globals() and emb_best_oof is not None:\n",
    "    def best_weight_for_blend(y, p_lex, p_emb, steps=101):\n",
    "        best_w, best_loss = 0.5, 1e9\n",
    "        for i in range(steps):\n",
    "            w = i/(steps-1)\n",
    "            blend = w*p_lex + (1-w)*p_emb\n",
    "            loss = log_loss(y, blend, labels=classes)\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss; best_w = w\n",
    "        return best_w, best_loss\n",
    "\n",
    "    w_blend, loss_blend = best_weight_for_blend(y, lex_best_oof, emb_best_oof)\n",
    "    print(f'Ensemble best weight (lexical): {w_blend:.2f}, OOF log_loss: {loss_blend:.5f}')\n",
    "else:\n",
    "    # Fallback: no embeddings available; use lexical-only\n",
    "    w_blend, loss_blend = 1.0, log_loss(y, lex_best_oof, labels=classes)\n",
    "    print(f'Embeddings unavailable; ensemble defaults to lexical only. OOF log_loss (lex): {loss_blend:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e491d",
   "metadata": {},
   "source": [
    "## Optional: Lightweight LoRA fine-tuning with temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c07fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T19:36:42.461190Z",
     "iopub.status.busy": "2025-11-04T19:36:42.460737Z",
     "iopub.status.idle": "2025-11-04T19:36:42.475887Z",
     "shell.execute_reply": "2025-11-04T19:36:42.474930Z",
     "shell.execute_reply.started": "2025-11-04T19:36:42.461170Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LoRA training (RUN_LORA=False).\n"
     ]
    }
   ],
   "source": [
    "RUN_LORA = False  # set True to train (can be slow; requires CUDA for practical speed)\n",
    "# For offline runs, point this to a local folder (Kaggle dataset) containing the model files\n",
    "import os\n",
    "LORA_MODEL_NAME = os.environ.get('LORA_MODEL_PATH', '/kaggle/input/distilbert-base-uncased')\n",
    "\n",
    "lora_oof = None\n",
    "lora_test_proba = None\n",
    "\n",
    "if RUN_LORA:\n",
    "    # If offline and local folder missing, skip LoRA gracefully\n",
    "    if not os.path.isdir(LORA_MODEL_NAME):\n",
    "        print(f\"LoRA model folder not found at '{LORA_MODEL_NAME}'. Attach the model as a Kaggle dataset or set LORA_MODEL_PATH env var. Skipping LoRA.\")\n",
    "    else:\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "        from peft import LoraConfig, get_peft_model\n",
    "        from datasets import Dataset\n",
    "\n",
    "        device = DEVICE  # from setup cell\n",
    "\n",
    "        # Build text inputs for classification (prompt + A + B)\n",
    "        def build_input(df):\n",
    "            return (\n",
    "                '[PROMPT] ' + df['prompt_text'] +\n",
    "                ' [A] ' + df['response_a_text'] +\n",
    "                ' [B] ' + df['response_b_text']\n",
    "            )\n",
    "\n",
    "        train_inputs = build_input(train_df)\n",
    "        test_inputs  = build_input(test_df)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_NAME, use_fast=True, local_files_only=True)\n",
    "\n",
    "        def tokenize_fn(batch):\n",
    "            return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "        ds_train = Dataset.from_pandas(pd.DataFrame({'text': train_inputs, 'label': y}))\n",
    "        ds_test  = Dataset.from_pandas(pd.DataFrame({'text': test_inputs}))\n",
    "\n",
    "        # Use half precision on CUDA to speed up training/inference\n",
    "        torch_dtype = torch.float16 if USE_CUDA else torch.float32\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL_NAME, num_labels=3, torch_dtype=torch_dtype, local_files_only=True)\n",
    "\n",
    "        # Auto-detect common attention module names for LoRA targets\n",
    "        target_keywords = ['q_proj','v_proj','k_proj','query','key','value','q_lin','v_lin']\n",
    "        all_module_names = [n for n,_ in model.named_modules()]\n",
    "        target_modules = sorted({n.split('.')[-1] for n in all_module_names if any(k in n for k in target_keywords)})\n",
    "        if not target_modules:\n",
    "            # fallback for DistilBERT attention names\n",
    "            target_modules = ['q_lin','v_lin']\n",
    "\n",
    "        peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, task_type='SEQ_CLS', target_modules=target_modules)\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.to(device)\n",
    "\n",
    "        tokenized_train = ds_train.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "        tokenized_test  = ds_test.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "            return {'log_loss': log_loss(labels, probs, labels=classes)}\n",
    "\n",
    "        # Simple split for evaluation and temperature fitting\n",
    "        tr_idx, va_idx = train_test_split(np.arange(len(ds_train)), test_size=0.15, random_state=RANDOM_STATE, stratify=y)\n",
    "        ds_tr = tokenized_train.select(tr_idx.tolist())\n",
    "        ds_va = tokenized_train.select(va_idx.tolist())\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir='out_lora',\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-4,\n",
    "            evaluation_strategy='steps',\n",
    "            eval_steps=200,\n",
    "            logging_steps=100,\n",
    "            save_strategy='no',\n",
    "            fp16=USE_CUDA,\n",
    "            dataloader_pin_memory=USE_CUDA,\n",
    "            report_to=[]\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, compute_metrics=compute_metrics)\n",
    "        trainer.train()\n",
    "\n",
    "        # Temperature scaling on validation logits\n",
    "        with torch.no_grad():\n",
    "            va_logits = torch.tensor(trainer.predict(ds_va).predictions, device=device)\n",
    "            va_labels = torch.tensor(y[va_idx], device=device)\n",
    "\n",
    "        temperature = torch.nn.Parameter(torch.ones((), device=device))\n",
    "        opt = torch.optim.LBFGS([temperature], lr=0.1, max_iter=50)\n",
    "\n",
    "        def nll_with_temperature():\n",
    "            opt.zero_grad()\n",
    "            scaled = va_logits / temperature.clamp_min(1e-3)\n",
    "            loss = torch.nn.functional.cross_entropy(scaled, va_labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        opt.step(nll_with_temperature)\n",
    "        T = float(temperature.detach().cpu().numpy())\n",
    "        print(f'Fitted temperature: {T:.3f}')\n",
    "\n",
    "        # OOF-like predictions via simple CV (one split used above); approximate OOF by combining tr/va\n",
    "        # For simplicity we will treat validation as OOF and train part as model predictions on train subset.\n",
    "        with torch.no_grad():\n",
    "            tr_logits = torch.tensor(trainer.predict(ds_tr).predictions, device=device)\n",
    "            tr_probs = torch.softmax(tr_logits / T, dim=1).cpu().numpy()\n",
    "            va_probs = torch.softmax(va_logits / T, dim=1).cpu().numpy()\n",
    "        lora_oof = np.zeros((len(train_df), 3), dtype=float)\n",
    "        lora_oof[tr_idx] = tr_probs\n",
    "        lora_oof[va_idx] = va_probs\n",
    "        print('LoRA pseudo-OOF log_loss:', log_loss(y, lora_oof, labels=classes))\n",
    "\n",
    "        # Test predictions\n",
    "        with torch.no_grad():\n",
    "            test_logits = torch.tensor(trainer.predict(tokenized_test).predictions, device=device)\n",
    "            lora_test_proba = torch.softmax(test_logits / T, dim=1).cpu().numpy()\n",
    "else:\n",
    "    print('Skipping LoRA training (RUN_LORA=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3c67c",
   "metadata": {},
   "source": [
    "## Fit final models on full data and produce submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1485d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T19:36:47.459280Z",
     "iopub.status.busy": "2025-11-04T19:36:47.458584Z",
     "iopub.status.idle": "2025-11-04T19:36:47.520964Z",
     "shell.execute_reply": "2025-11-04T19:36:47.520040Z",
     "shell.execute_reply.started": "2025-11-04T19:36:47.459218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_step3_lexical_calibrated.csv\n",
      "Saved submission_step3_embeddings_calibrated.csv\n",
      "Saved submission_step3_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Lexical: choose best calibration and train on full data\n",
    "lex_best_models = lex_sigmoid_models if lex_best_name=='sigmoid' else lex_isotonic_models\n",
    "lex_best_scalers = lex_sigmoid_scalers if lex_best_name=='sigmoid' else lex_isotonic_scalers\n",
    "# Refit: use all folds' models+scalers to average predictions on test\n",
    "lex_proba_test_list = []\n",
    "for clf, scaler in zip(lex_best_models, lex_best_scalers):\n",
    "    Xs = scaler.transform(X_lex_test)\n",
    "    lex_proba_test_list.append(clf.predict_proba(Xs))\n",
    "lex_proba_test = np.mean(lex_proba_test_list, axis=0)\n",
    "\n",
    "sub_lex = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': lex_proba_test[:,0],\n",
    "    'winner_model_b': lex_proba_test[:,1],\n",
    "    'winner_tie':     lex_proba_test[:,2],\n",
    "})\n",
    "sub_lex.to_csv('submission_step3_lexical_calibrated.csv', index=False)\n",
    "print('Saved submission_step3_lexical_calibrated.csv')\n",
    "\n",
    "# 2) Embeddings: choose best calibration and train on full data (if available)\n",
    "emb_proba_test = None\n",
    "if 'emb_best_name' in globals() and emb_best_name is not None and X_emb_test is not None:\n",
    "    emb_best_models = emb_sigmoid_models if emb_best_name=='sigmoid' else emb_isotonic_models\n",
    "    emb_best_scalers = emb_sigmoid_scalers if emb_best_name=='sigmoid' else emb_isotonic_scalers\n",
    "    emb_proba_test_list = []\n",
    "    for clf, scaler in zip(emb_best_models, emb_best_scalers):\n",
    "        Xs = scaler.transform(X_emb_test)\n",
    "        emb_proba_test_list.append(clf.predict_proba(Xs))\n",
    "    emb_proba_test = np.mean(emb_proba_test_list, axis=0)\n",
    "\n",
    "    sub_emb = pd.DataFrame({\n",
    "        'id': test_df['id'].values,\n",
    "        'winner_model_a': emb_proba_test[:,0],\n",
    "        'winner_model_b': emb_proba_test[:,1],\n",
    "        'winner_tie':     emb_proba_test[:,2],\n",
    "    })\n",
    "    sub_emb.to_csv('submission_step3_embeddings_calibrated.csv', index=False)\n",
    "    print('Saved submission_step3_embeddings_calibrated.csv')\n",
    "else:\n",
    "    print('Skipping embeddings submission (embeddings unavailable).')\n",
    "\n",
    "# 3) LoRA submission if available\n",
    "if lora_test_proba is not None:\n",
    "    sub_lora = pd.DataFrame({\n",
    "        'id': test_df['id'].values,\n",
    "        'winner_model_a': lora_test_proba[:,0],\n",
    "        'winner_model_b': lora_test_proba[:,1],\n",
    "        'winner_tie':     lora_test_proba[:,2],\n",
    "    })\n",
    "    sub_lora.to_csv('submission_step3_lora.csv', index=False)\n",
    "    print('Saved submission_step3_lora.csv')\n",
    "\n",
    "# 4) Ensemble using OOF-optimal weight between lexical and embeddings (if embeddings exist)\n",
    "if emb_proba_test is not None:\n",
    "    blend_test = w_blend * lex_proba_test + (1 - w_blend) * emb_proba_test\n",
    "    sub_blend = pd.DataFrame({\n",
    "        'id': test_df['id'].values,\n",
    "        'winner_model_a': blend_test[:,0],\n",
    "        'winner_model_b': blend_test[:,1],\n",
    "        'winner_tie':     blend_test[:,2],\n",
    "    })\n",
    "    sub_blend.to_csv('submission_step3_ensemble.csv', index=False)\n",
    "    print('Saved submission_step3_ensemble.csv')\n",
    "else:\n",
    "    # Optional: also write an ensemble file identical to lexical for convenience\n",
    "    sub_blend = sub_lex.copy()\n",
    "    sub_blend.to_csv('submission_step3_ensemble.csv', index=False)\n",
    "    print('Embeddings missing; wrote submission_step3_ensemble.csv identical to lexical.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f9d9d",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- To enable LoRA fine-tuning, set `RUN_LORA = True` in the LoRA cell.\n",
    "- LoRA section uses PEFT; ensure `peft`, `transformers`, `datasets`, and `torch` are installed.\n",
    "- Calibrated models use scikit-learn's `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods tested via OOF.\n",
    "- The ensemble weight is found by minimizing OOF log_loss over a simple 1D grid.\n",
    "- All submissions are written to the working directory."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "isSourceIdPinned": false,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 490520,
     "modelInstanceId": 474627,
     "sourceId": 629952,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
