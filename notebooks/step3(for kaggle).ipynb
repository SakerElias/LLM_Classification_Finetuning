{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":629952,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":474627,"modelId":490520}],"dockerImageVersionId":31154,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a23592e9","cell_type":"markdown","source":"# Step 3 — Extensions: Bias features, Calibration, Ensembling, and LoRA fine-tuning\n\nThis notebook implements the full Step 3 pipeline without touching prior notebooks:\n\n- Bias-aware lexical features (verbosity and structure)\n- Calibrated classifiers (sigmoid and isotonic)\n- Embeddings-based model (reusing precomputed .npy when available)\n- Simple ensembling via OOF-weight search\n- Optional lightweight LoRA fine-tuning with temperature scaling\n\nOutputs: submission CSVs for each component and a blended ensemble.","metadata":{}},{"id":"61179c45","cell_type":"code","source":"# Imports & setup\nimport os, re, ast, json, math, random, warnings\nfrom pathlib import Path\nimport numpy as np, pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import log_loss\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nwarnings.filterwarnings('ignore')\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\nrandom.seed(RANDOM_STATE)\n\nTRAIN_PATH = '/kaggle/input/llm-classification-finetuning/train.csv'\nTEST_PATH = '/kaggle/input/llm-classification-finetuning/test.csv'\n\n\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df  = pd.read_csv(TEST_PATH)\n\n# Build 3-class target consistent with previous notebooks: 0=A, 1=B, 2=Tie\ny = np.select([train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1], [0,1,2])\ntrain_df['target'] = y\nclasses = [0,1,2]  # ensure consistent ordering for log_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:55:54.939113Z","iopub.execute_input":"2025-11-04T18:55:54.939360Z","iopub.status.idle":"2025-11-04T18:55:56.799542Z","shell.execute_reply.started":"2025-11-04T18:55:54.939345Z","shell.execute_reply":"2025-11-04T18:55:56.798940Z"}},"outputs":[],"execution_count":11},{"id":"aee06ef1","cell_type":"code","source":"# Text extraction utilities (same behavior as earlier notebook)\ndef extract_text_from_field(text_field):\n    try:\n        parsed = ast.literal_eval(text_field)\n        return ' '.join(parsed) if isinstance(parsed, list) else str(parsed)\n    except Exception:\n        return str(text_field)\n\nfor df in (train_df, test_df):\n    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n    df['text_a'] = df['prompt_text'] + ' [SEP] ' + df['response_a_text']\n    df['text_b'] = df['prompt_text'] + ' [SEP] ' + df['response_b_text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:55:56.800894Z","iopub.execute_input":"2025-11-04T18:55:56.801277Z","iopub.status.idle":"2025-11-04T18:56:00.685936Z","shell.execute_reply.started":"2025-11-04T18:55:56.801253Z","shell.execute_reply":"2025-11-04T18:56:00.685327Z"}},"outputs":[],"execution_count":12},{"id":"d7a26d75","cell_type":"markdown","source":"## Bias-aware and structural lexical features","metadata":{}},{"id":"57487ed8","cell_type":"code","source":"# Structural counters\n\ndef count_pattern(text, pattern):\n    if not isinstance(text, str):\n        return 0\n    return len(re.findall(pattern, text))\n\n\ndef paragraph_count(t):\n    return t.count('\\n\\n') if isinstance(t, str) else 0\n\n\ndef list_count(t):\n    return count_pattern(t, r'(^\\s*[\\-\\*•]\\s|\\d+\\.)')\n\n\ndef quote_count(t):\n    return count_pattern(t, r'>|\\*\\*')\n\n\ndef sentence_count(t):\n    return count_pattern(t, r'[.!?](\\s|$)')\n\n\ndef code_block_count(t):\n    return count_pattern(t, r'```|`[^`]+`')\n\n\ndef heading_count(t):\n    return count_pattern(t, r'^(#|##|###|####|#####|######)\\s')\n\n\ndef word_count(t):\n    return len(t.split()) if isinstance(t, str) else 0\n\n# Compute per-side features using explicit suffix mapping ('a' / 'b')\nfor df in (train_df, test_df):\n    for col, suffix in [('response_a_text', 'a'), ('response_b_text', 'b')]:\n        df[f'len_{suffix}']   = df[col].astype(str).apply(len)\n        df[f'wc_{suffix}']    = df[col].astype(str).apply(word_count)\n        df[f'sent_{suffix}']  = df[col].apply(sentence_count)\n        df[f'para_{suffix}']  = df[col].apply(paragraph_count)\n        df[f'list_{suffix}']  = df[col].apply(list_count)\n        df[f'quote_{suffix}'] = df[col].apply(quote_count)\n        df[f'code_{suffix}']  = df[col].apply(code_block_count)\n        df[f'hdr_{suffix}']   = df[col].apply(heading_count)\n\n    # Diffs (A - B) — captures verbosity and structure bias\n    for base in ['len','wc','sent','para','list','quote','code','hdr']:\n        df[f'{base}_diff'] = df[f'{base}_a'] - df[f'{base}_b']\n\n    # Ratios (A / (B+1)) to capture scale-invariant verbosity bias\n    for base in ['len','wc','sent']:\n        df[f'{base}_ratio'] = df[f'{base}_a'] / (df[f'{base}_b'] + 1.0)\n\nLEX_FEATURES = [\n    'len_diff','wc_diff','sent_diff','para_diff','list_diff','quote_diff','code_diff','hdr_diff',\n    'len_ratio','wc_ratio','sent_ratio'\n]\nX_lex = train_df[LEX_FEATURES].fillna(0).astype(float)\nX_lex_test = test_df[LEX_FEATURES].fillna(0).astype(float)\nX_lex.shape, X_lex_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:56:00.686604Z","iopub.execute_input":"2025-11-04T18:56:00.686818Z","iopub.status.idle":"2025-11-04T18:56:11.132925Z","shell.execute_reply.started":"2025-11-04T18:56:00.686802Z","shell.execute_reply":"2025-11-04T18:56:11.132157Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"((57477, 11), (3, 11))"},"metadata":{}}],"execution_count":13},{"id":"1ab1f8a7","cell_type":"markdown","source":"## Calibrated lexical model (sigmoid and isotonic) with OOF predictions","metadata":{}},{"id":"a9917e85","cell_type":"code","source":"def cv_calibrated_probs(X, y, base_model=None, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n    if base_model is None:\n        base_model = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.zeros((len(X), 3), dtype=float)\n    models = []\n    scalers = []\n    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n        Xtr, Xva = X.iloc[tr], X.iloc[va]\n        ytr, yva = y[tr], y[va]\n        scaler = StandardScaler()\n        Xtr_s = scaler.fit_transform(Xtr)\n        Xva_s = scaler.transform(Xva)\n        clf = CalibratedClassifierCV(estimator=base_model, method=method, cv=3)\n        clf.fit(Xtr_s, ytr)\n        proba = clf.predict_proba(Xva_s)\n        oof[va] = proba\n        loss = log_loss(yva, proba, labels=classes)\n        print(f'[Lex {method}] Fold {fold}: log_loss={loss:.5f}')\n        models.append(clf)\n        scalers.append(scaler)\n    print(f'[Lex {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n    return oof, models, scalers\n\noof_lex_sigmoid, lex_sigmoid_models, lex_sigmoid_scalers = cv_calibrated_probs(X_lex, y, method='sigmoid')\noof_lex_isotonic, lex_isotonic_models, lex_isotonic_scalers = cv_calibrated_probs(X_lex, y, method='isotonic')\n\n# Choose the better calibration by OOF loss\nlex_oof_list = [('sigmoid', oof_lex_sigmoid), ('isotonic', oof_lex_isotonic)]\nlex_best_name, lex_best_oof = min(lex_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\nprint('Best lexical calibration:', lex_best_name, 'OOF log_loss=', log_loss(y, lex_best_oof, labels=classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T18:56:11.133736Z","iopub.execute_input":"2025-11-04T18:56:11.134042Z","iopub.status.idle":"2025-11-04T18:56:23.768408Z","shell.execute_reply.started":"2025-11-04T18:56:11.134019Z","shell.execute_reply":"2025-11-04T18:56:23.767703Z"}},"outputs":[{"name":"stdout","text":"[Lex sigmoid] Fold 1: log_loss=1.06936\n[Lex sigmoid] Fold 2: log_loss=1.06919\n[Lex sigmoid] Fold 3: log_loss=1.07031\n[Lex sigmoid] Fold 4: log_loss=1.07064\n[Lex sigmoid] Fold 5: log_loss=1.06862\n[Lex sigmoid] OOF log_loss: 1.06962\n[Lex isotonic] Fold 1: log_loss=1.06081\n[Lex isotonic] Fold 2: log_loss=1.06201\n[Lex isotonic] Fold 3: log_loss=1.06128\n[Lex isotonic] Fold 4: log_loss=1.05980\n[Lex isotonic] Fold 5: log_loss=1.05939\n[Lex isotonic] OOF log_loss: 1.06066\nBest lexical calibration: isotonic OOF log_loss= 1.060656514215499\n","output_type":"stream"}],"execution_count":14},{"id":"b6260c41","cell_type":"markdown","source":"## Embedding features (reuse precomputed .npy if available)","metadata":{}},{"id":"9e91be0c","cell_type":"code","source":"EMBED_A_TRAIN = Path('/kaggle/input/embeddings/pytorch/default/1/train_embeddings_a.npy')\nEMBED_B_TRAIN = Path('/kaggle/input/embeddings/pytorch/default/1/train_embeddings_b.npy')\nEMBED_A_TEST  = Path('/kaggle/input/embeddings/pytorch/default/1/test_embeddings_a.npy')\nEMBED_B_TEST  = Path('/kaggle/input/embeddings/pytorch/default/1/test_embeddings_b.npy')\n\ndef ensure_embeddings(train_df, test_df, batch_size=32, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer(model_name)\n    train_a = model.encode(train_df['text_a'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n    train_b = model.encode(train_df['text_b'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n    test_a  = model.encode(test_df['text_a'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n    test_b  = model.encode(test_df['text_b'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n    np.save(EMBED_A_TRAIN, train_a); np.save(EMBED_B_TRAIN, train_b)\n    np.save(EMBED_A_TEST,  test_a);  np.save(EMBED_B_TEST,  test_b)\n    return train_a, train_b, test_a, test_b\n\nif EMBED_A_TRAIN.exists() and EMBED_B_TRAIN.exists() and EMBED_A_TEST.exists() and EMBED_B_TEST.exists():\n    train_a = np.load(EMBED_A_TRAIN)\n    train_b = np.load(EMBED_B_TRAIN)\n    test_a  = np.load(EMBED_A_TEST)\n    test_b  = np.load(EMBED_B_TEST)\nelse:\n    train_a, train_b, test_a, test_b = ensure_embeddings(train_df, test_df)\n\nX_emb = np.concatenate([train_a, train_b], axis=1)\nX_emb_test = np.concatenate([test_a, test_b], axis=1)\nX_emb.shape, X_emb_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:25:07.363397Z","iopub.execute_input":"2025-11-04T19:25:07.364191Z","iopub.status.idle":"2025-11-04T19:25:07.514406Z","shell.execute_reply.started":"2025-11-04T19:25:07.364162Z","shell.execute_reply":"2025-11-04T19:25:07.513722Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"((57477, 768), (3, 768))"},"metadata":{}}],"execution_count":20},{"id":"95d18348","cell_type":"markdown","source":"## Calibrated embeddings model with OOF predictions","metadata":{}},{"id":"df93cef7","cell_type":"code","source":"def cv_calibrated_probs_numpy(X, y, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n    oof = np.zeros((len(X), 3), dtype=float)\n    models, scalers = [], []\n    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n        Xtr, Xva = X[tr], X[va]\n        ytr, yva = y[tr], y[va]\n        scaler = StandardScaler()\n        Xtr_s = scaler.fit_transform(Xtr)\n        Xva_s = scaler.transform(Xva)\n        base = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n        clf = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n        clf.fit(Xtr_s, ytr)\n        proba = clf.predict_proba(Xva_s)\n        oof[va] = proba\n        loss = log_loss(yva, proba, labels=classes)\n        print(f'[Emb {method}] Fold {fold}: log_loss={loss:.5f}')\n        models.append(clf); scalers.append(scaler)\n    print(f'[Emb {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n    return oof, models, scalers\n\noof_emb_sigmoid, emb_sigmoid_models, emb_sigmoid_scalers = cv_calibrated_probs_numpy(X_emb, y, method='sigmoid')\noof_emb_isotonic, emb_isotonic_models, emb_isotonic_scalers = cv_calibrated_probs_numpy(X_emb, y, method='isotonic')\n\nemb_oof_list = [('sigmoid', oof_emb_sigmoid), ('isotonic', oof_emb_isotonic)]\nemb_best_name, emb_best_oof = min(emb_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\nprint('Best embedding calibration:', emb_best_name, 'OOF log_loss=', log_loss(y, emb_best_oof, labels=classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:25:10.921449Z","iopub.execute_input":"2025-11-04T19:25:10.921780Z","iopub.status.idle":"2025-11-04T19:35:02.234884Z","shell.execute_reply.started":"2025-11-04T19:25:10.921758Z","shell.execute_reply":"2025-11-04T19:35:02.234017Z"}},"outputs":[{"name":"stdout","text":"[Emb sigmoid] Fold 1: log_loss=1.06656\n[Emb sigmoid] Fold 2: log_loss=1.06945\n[Emb sigmoid] Fold 3: log_loss=1.06616\n[Emb sigmoid] Fold 4: log_loss=1.06705\n[Emb sigmoid] Fold 5: log_loss=1.06736\n[Emb sigmoid] OOF log_loss: 1.06732\n[Emb isotonic] Fold 1: log_loss=1.06543\n[Emb isotonic] Fold 2: log_loss=1.07141\n[Emb isotonic] Fold 3: log_loss=1.06569\n[Emb isotonic] Fold 4: log_loss=1.06628\n[Emb isotonic] Fold 5: log_loss=1.06670\n[Emb isotonic] OOF log_loss: 1.06710\nBest embedding calibration: isotonic OOF log_loss= 1.0671003095562928\n","output_type":"stream"}],"execution_count":21},{"id":"b8c54b32","cell_type":"markdown","source":"## Simple ensemble (OOF-weight search on lexical + embeddings)","metadata":{}},{"id":"cf63b33c","cell_type":"code","source":"# Grid search weights w in [0..1] for p = w*p_lex + (1-w)*p_emb minimizing OOF log_loss\ndef best_weight_for_blend(y, p_lex, p_emb, steps=101):\n    best_w, best_loss = 0.5, 1e9\n    for i in range(steps):\n        w = i/(steps-1)\n        blend = w*p_lex + (1-w)*p_emb\n        loss = log_loss(y, blend, labels=classes)\n        if loss < best_loss:\n            best_loss = loss; best_w = w\n    return best_w, best_loss\n\nw_blend, loss_blend = best_weight_for_blend(y, lex_best_oof, emb_best_oof)\nprint(f'Ensemble best weight (lexical): {w_blend:.2f}, OOF log_loss: {loss_blend:.5f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:36:37.842136Z","iopub.execute_input":"2025-11-04T19:36:37.842443Z","iopub.status.idle":"2025-11-04T19:36:38.772010Z","shell.execute_reply.started":"2025-11-04T19:36:37.842422Z","shell.execute_reply":"2025-11-04T19:36:38.771255Z"}},"outputs":[{"name":"stdout","text":"Ensemble best weight (lexical): 0.59, OOF log_loss: 1.05406\n","output_type":"stream"}],"execution_count":22},{"id":"da7e491d","cell_type":"markdown","source":"## Optional: Lightweight LoRA fine-tuning with temperature scaling","metadata":{}},{"id":"a10c07fc","cell_type":"code","source":"RUN_LORA = False  # set True to train (can be slow)\nLORA_MODEL_NAME = 'distilbert-base-uncased'  # small and fast; change to 'microsoft/deberta-v3-small' if desired\n\nlora_oof = None\nlora_test_proba = None\n\nif RUN_LORA:\n    import torch\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n    from peft import LoraConfig, get_peft_model\n    from datasets import Dataset\n\n    # Build text inputs for classification (prompt + A + B)\n    def build_input(df):\n        return (\n            '[PROMPT] ' + df['prompt_text'] +\n            ' [A] ' + df['response_a_text'] +\n            ' [B] ' + df['response_b_text']\n        )\n\n    train_inputs = build_input(train_df)\n    test_inputs  = build_input(test_df)\n\n    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_NAME)\n\n    def tokenize_fn(batch):\n        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=512)\n\n    ds_train = Dataset.from_pandas(pd.DataFrame({'text': train_inputs, 'label': y}))\n    ds_test  = Dataset.from_pandas(pd.DataFrame({'text': test_inputs}))\n\n    model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL_NAME, num_labels=3)\n\n    # Auto-detect common attention module names for LoRA targets\n    target_keywords = ['q_proj','v_proj','k_proj','query','key','value','q_lin','v_lin']\n    all_module_names = [n for n,_ in model.named_modules()]\n    target_modules = sorted({n.split('.')[-1] for n in all_module_names if any(k in n for k in target_keywords)})\n    if not target_modules:\n        # fallback for DistilBERT attention names\n        target_modules = ['q_lin','v_lin']\n\n    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, task_type='SEQ_CLS', target_modules=target_modules)\n    model = get_peft_model(model, peft_config)\n\n    tokenized_train = ds_train.map(tokenize_fn, batched=True, remove_columns=['text'])\n    tokenized_test  = ds_test.map(tokenize_fn, batched=True, remove_columns=['text'])\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n        return {'log_loss': log_loss(labels, probs, labels=classes)}\n\n    # Simple split for evaluation and temperature fitting\n    tr_idx, va_idx = train_test_split(np.arange(len(ds_train)), test_size=0.15, random_state=RANDOM_STATE, stratify=y)\n    ds_tr = tokenized_train.select(tr_idx.tolist())\n    ds_va = tokenized_train.select(va_idx.tolist())\n\n    args = TrainingArguments(\n        output_dir='out_lora',\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        num_train_epochs=1,\n        learning_rate=2e-4,\n        evaluation_strategy='steps',\n        eval_steps=200,\n        logging_steps=100,\n        save_strategy='no',\n        report_to=[]\n    )\n\n    trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, compute_metrics=compute_metrics)\n    trainer.train()\n\n    # Temperature scaling on validation logits\n    with torch.no_grad():\n        va_logits = torch.tensor(trainer.predict(ds_va).predictions)\n        va_labels = torch.tensor(y[va_idx])\n\n    temperature = torch.nn.Parameter(torch.ones(()))\n    opt = torch.optim.LBFGS([temperature], lr=0.1, max_iter=50)\n\n    def nll_with_temperature():\n        opt.zero_grad()\n        scaled = va_logits / temperature.clamp_min(1e-3)\n        loss = torch.nn.functional.cross_entropy(scaled, va_labels)\n        loss.backward()\n        return loss\n\n    opt.step(nll_with_temperature)\n    T = float(temperature.detach().cpu().numpy())\n    print(f'Fitted temperature: {T:.3f}')\n\n    # OOF-like predictions via simple CV (one split used above); approximate OOF by combining tr/va\n    # For simplicity we will treat validation as OOF and train part as model predictions on train subset.\n    with torch.no_grad():\n        tr_logits = torch.tensor(trainer.predict(ds_tr).predictions)\n        tr_probs = torch.softmax(tr_logits / T, dim=1).numpy()\n        va_probs = torch.softmax(va_logits / T, dim=1).numpy()\n    lora_oof = np.zeros((len(train_df), 3), dtype=float)\n    lora_oof[tr_idx] = tr_probs\n    lora_oof[va_idx] = va_probs\n    print('LoRA pseudo-OOF log_loss:', log_loss(y, lora_oof, labels=classes))\n\n    # Test predictions\n    with torch.no_grad():\n        test_logits = torch.tensor(trainer.predict(tokenized_test).predictions)\n        lora_test_proba = torch.softmax(test_logits / T, dim=1).numpy()\nelse:\n    print('Skipping LoRA training (RUN_LORA=False).')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:36:42.460737Z","iopub.execute_input":"2025-11-04T19:36:42.461190Z","iopub.status.idle":"2025-11-04T19:36:42.475887Z","shell.execute_reply.started":"2025-11-04T19:36:42.461170Z","shell.execute_reply":"2025-11-04T19:36:42.474930Z"}},"outputs":[{"name":"stdout","text":"Skipping LoRA training (RUN_LORA=False).\n","output_type":"stream"}],"execution_count":23},{"id":"26c3c67c","cell_type":"markdown","source":"## Fit final models on full data and produce submissions","metadata":{}},{"id":"8b1485d3","cell_type":"code","source":"# 1) Lexical: choose best calibration and train on full data\nlex_best_models = lex_sigmoid_models if lex_best_name=='sigmoid' else lex_isotonic_models\nlex_best_scalers = lex_sigmoid_scalers if lex_best_name=='sigmoid' else lex_isotonic_scalers\n# Refit: use all folds' models+scalers to average predictions on test\nlex_proba_test_list = []\nfor clf, scaler in zip(lex_best_models, lex_best_scalers):\n    Xs = scaler.transform(X_lex_test)\n    lex_proba_test_list.append(clf.predict_proba(Xs))\nlex_proba_test = np.mean(lex_proba_test_list, axis=0)\n\nsub_lex = pd.DataFrame({\n    'id': test_df['id'].values,\n    'winner_model_a': lex_proba_test[:,0],\n    'winner_model_b': lex_proba_test[:,1],\n    'winner_tie':     lex_proba_test[:,2],\n})\nsub_lex.to_csv('submission_step3_lexical_calibrated.csv', index=False)\nprint('Saved submission_step3_lexical_calibrated.csv')\n\n# 2) Embeddings: choose best calibration and train on full data\nemb_best_models = emb_sigmoid_models if emb_best_name=='sigmoid' else emb_isotonic_models\nemb_best_scalers = emb_sigmoid_scalers if emb_best_name=='sigmoid' else emb_isotonic_scalers\nemb_proba_test_list = []\nfor clf, scaler in zip(emb_best_models, emb_best_scalers):\n    Xs = scaler.transform(X_emb_test)\n    emb_proba_test_list.append(clf.predict_proba(Xs))\nemb_proba_test = np.mean(emb_proba_test_list, axis=0)\n\nsub_emb = pd.DataFrame({\n    'id': test_df['id'].values,\n    'winner_model_a': emb_proba_test[:,0],\n    'winner_model_b': emb_proba_test[:,1],\n    'winner_tie':     emb_proba_test[:,2],\n})\nsub_emb.to_csv('submission_step3_embeddings_calibrated.csv', index=False)\nprint('Saved submission_step3_embeddings_calibrated.csv')\n\n# 3) LoRA submission if available\nif lora_test_proba is not None:\n    sub_lora = pd.DataFrame({\n        'id': test_df['id'].values,\n        'winner_model_a': lora_test_proba[:,0],\n        'winner_model_b': lora_test_proba[:,1],\n        'winner_tie':     lora_test_proba[:,2],\n    })\n    sub_lora.to_csv('submission_step3_lora.csv', index=False)\n    print('Saved submission_step3_lora.csv')\n\n# 4) Ensemble using OOF-optimal weight between lexical and embeddings\nblend_test = w_blend * lex_proba_test + (1 - w_blend) * emb_proba_test\nsub_blend = pd.DataFrame({\n    'id': test_df['id'].values,\n    'winner_model_a': blend_test[:,0],\n    'winner_model_b': blend_test[:,1],\n    'winner_tie':     blend_test[:,2],\n})\nsub_blend.to_csv('submission_step3_ensemble.csv', index=False)\nprint('Saved submission_step3_ensemble.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:36:47.458584Z","iopub.execute_input":"2025-11-04T19:36:47.459280Z","iopub.status.idle":"2025-11-04T19:36:47.520964Z","shell.execute_reply.started":"2025-11-04T19:36:47.459218Z","shell.execute_reply":"2025-11-04T19:36:47.520040Z"}},"outputs":[{"name":"stdout","text":"Saved submission_step3_lexical_calibrated.csv\nSaved submission_step3_embeddings_calibrated.csv\nSaved submission_step3_ensemble.csv\n","output_type":"stream"}],"execution_count":24},{"id":"5e2f9d9d","cell_type":"markdown","source":"### Notes\n- To enable LoRA fine-tuning, set `RUN_LORA = True` in the LoRA cell.\n- LoRA section uses PEFT; ensure `peft`, `transformers`, `datasets`, and `torch` are installed.\n- Calibrated models use scikit-learn's `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods tested via OOF.\n- The ensemble weight is found by minimizing OOF log_loss over a simple 1D grid.\n- All submissions are written to the working directory.","metadata":{}}]}