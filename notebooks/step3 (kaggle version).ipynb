{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:50.640482Z",
     "iopub.status.busy": "2025-11-05T22:27:50.640220Z",
     "iopub.status.idle": "2025-11-05T22:27:51.053356Z",
     "shell.execute_reply": "2025-11-05T22:27:51.052757Z",
     "shell.execute_reply.started": "2025-11-05T22:27:50.640461Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/embeddings/other/default/1/train_embeddings_a.npy\n",
      "/kaggle/input/embeddings/other/default/1/train_embeddings_b.npy\n",
      "/kaggle/input/embeddings/other/default/1/test_embeddings_a.npy\n",
      "/kaggle/input/embeddings/other/default/1/test_embeddings_b.npy\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/rust_model.ot\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/README.md\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tokenizer.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tf_model.h5\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/data_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/train_script.py\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tokenizer_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/sentence_bert_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/pytorch_model.bin\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/config_sentence_transformers.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/model.safetensors\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/modules.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/special_tokens_map.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.gitattributes\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/vocab.txt\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/1_Pooling/config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/onnx/model.onnx\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/config\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/packed-refs\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/index\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/description\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/info/exclude\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/refs/heads/main\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-merge\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-push\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/update.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-push.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-commit\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-checkout\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/push-to-checkout.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-update.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/53/aa/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/c3/a8/c3a85f238711653950f6a79ece63eb0ea93d76f6a6284be04019c53733baf256\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/2d/98/2d98d96d278348988f2744e6445b8bc16d921c3f6e17c667362f3cb353007aea\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/c0/1c/c01c3fb6f89996e15c1cb92ea3fa9cc3c76e9f385ed0b88ca71aebfad2be1547\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/24/c0/24c06a7429b843d46e40c6b167122053921bf94dce2e5550ea5c07fabc597646\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/refs/heads/main\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c7/9f2b6a0cea6f4b564fed1938984bace9d30ff0\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/61/23922cd0a37e85554070835282693f77c2cdfd\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/bf/12db93776dfe464438f7aa49e5286dc1f6a1d3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2e/85f0eac205cf444bdf97ede4935603ca6a0416\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/64/629dafb9c2f452be5264e1b39ca07703c285cb\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ff/e79e29723a82be6e47be2645d46f5e6cb7d4c6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/f0/e720b4416e21613c5f66bd2c80941a1d104e34\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ac/bb28c8aa70f5503c85d6b90e8cd65606993a20\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/8b/3219a92973c328a8e22fadcfa821b5dc75636a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2c/fdf1ac2b3fc894d8239133174385c0baed9a17\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/38/d2c305133e9d034646e99db5533c6f36ebbafc\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7d/bbc90392e2f80f3d3c277d6e90027e55de9125\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/e7/b0375001f109a6b8873d756ad4f7bbb15fbaa5\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/e4/ce9877abf3edfe10b0d82785e83bdcb973e22e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/1a/310852cf8e58d22c5ebff537711d504ad4ad66\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/34/17920b056b9a3d4257fba7a8bae5c5ae2910d6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c2/1050a7ef692090620a6d037dd736908f9c7cf6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/88/2b870dcd23cc8168e1e49fe6ef74b99d327416\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/cb/202bfe2e3c98645018a6d12f182a434c9d3e02\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/72/b987fd805cfa2b58c4c8c952b274a11bfd5a00\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/13/afc4e1fcb2e8688c8b565e612d78bf0cc0a203\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/99/7ea4ddfcbcd445473fc757249fc971a80337c9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7f/56ac753f54f5e3cc9aff3c5ab77b090bd699d3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fb/140275c155a9c7c5a3b3e0e77a9e839594a938\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/d4/9eff1e4a6f0e3ba5069f21c6d20aad156dd2c9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/74/640519a949f5c678e1c29cfe92b2f0f5d41354\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c6/aecd063146daa28b47cfc2b0ec031d17f27ec4\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c4/360cc4f803de8692c8152724105929eb278b7d\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/d1/514c3162bbe87b343f565fadc62e6c06f04f03\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/3f/2fccdd494d71e84130e80d200194e53f030fc6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/95/2a9b81c0bfd99800fabf352f69c7ccd46c5e43\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/91/7c37d89f3927e303b0f3f2e565a460604819de\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/71/7413c64de70e37b55cf53c9cdff0e2d331fac3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/59/d594003bf59880a884c574bf88ef7555bb0202\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/59/0529fb2f5ca26d5fdbea2bb538d399ddcc79fd\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fe/bd144c5f6a5614481e9998377f85003d4c848f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7c/3e117e015025bc605669cc11546708105b6f97\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2b/f99854609538f90af03c921c41a0cbde670b2f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/a4/ce9694439b000ffd9ef4fe8becf6f62621689e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/1f/04b00315ed69102b46817616310699bd49e129\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/82/b47b6277499b8f17d139d0c651a6f961c06124\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/dd/d10b4b20ead19a8121a3d8b4369091fb408e1c\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/5f/dfc644dfd78c46a33c04abeedadf36e7c6d37b\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/5f/d10429389515d3e5cccdeda08cae5fea1ae82e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/46/605decb5369335a3847c9f41bb0b896c07dd1a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/46/66f08731cd88a74a36484a43f249a6bfede34b\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/a1/c250340fff88adddcf3cfeab206d898f5553b8\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/44/eb4044493a3c34bc6d7faae1a71ec76665ebc6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/9a/1874cfc81fe03bde4dca6bde406fb80a6ddb29\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/63/daf3d0630eaec6401eed4a9bfb1e115bf2ba3e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ad/0563d758cca630a0ac9d344a42e14a05780583\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/37/46fd5f4cfd46ae64fc781df53e7cbb7849eb62\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/f7/2a19c9599caf1f8944a10d363150a5d36ff06c\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/8c/fec92309f5626a223304af2423e332f6d31887\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/9c/c46352c6e0729b6d9a3ce65e4603a16a63a9f9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/b3/406917f3229edc5165ba222038d9bffe957a2f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/29/368ef8bd008d7fb711a5968e4029dae93d9587\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fd/1b291129c607e5d49799f87cb219b27f98acdf\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/93/45a404b1c759ba192be71bfe32bc304f7a94e7\n",
      "/kaggle/input/final-embeddings/train_embeddings_a.npy\n",
      "/kaggle/input/final-embeddings/train_embeddings_b.npy\n",
      "/kaggle/input/final-embeddings/test_embeddings_a.npy\n",
      "/kaggle/input/final-embeddings/test_embeddings_b.npy\n",
      "/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3/config.json\n",
      "/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3/tokenizer.json\n",
      "/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3/metadata.json\n",
      "/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3/model.weights.h5\n",
      "/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3/assets/tokenizer/vocabulary.txt\n",
      "/kaggle/input/llm-classification-finetuning/sample_submission.csv\n",
      "/kaggle/input/llm-classification-finetuning/train.csv\n",
      "/kaggle/input/llm-classification-finetuning/test.csv\n",
      "/kaggle/input/d/eliassaker/embeddings/train_embeddings_a.npy\n",
      "/kaggle/input/d/eliassaker/embeddings/train_embeddings_b.npy\n",
      "/kaggle/input/d/eliassaker/embeddings/test_embeddings_a.npy\n",
      "/kaggle/input/d/eliassaker/embeddings/test_embeddings_b.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'false'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os, shutil\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Force offline behavior for Hugging Face\n",
    "os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "\n",
    "# Avoid importing TF/JAX backends via transformers and quiet TF/XLA logs\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_TF\", \"1\")\n",
    "os.environ.setdefault(\"TRANSFORMERS_NO_FLAX\", \"1\")\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"3\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:51.054892Z",
     "iopub.status.busy": "2025-11-05T22:27:51.054491Z",
     "iopub.status.idle": "2025-11-05T22:27:51.059935Z",
     "shell.execute_reply": "2025-11-05T22:27:51.059141Z",
     "shell.execute_reply.started": "2025-11-05T22:27:51.054873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 0) CONFIG\n",
    "MODE = \"ensemble\"  # \"lexical\" | \"embeddings\" | \"embeddings_lora\" | \"ensemble\"\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "# Kaggle data\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"/kaggle/input/llm-classification-finetuning/train.csv\")\n",
    "TEST_PATH  = os.environ.get(\"TEST_PATH\",  \"/kaggle/input/llm-classification-finetuning/test.csv\")\n",
    "\n",
    "# Sentence-Transformers local folder (attach a dataset and point here)\n",
    "ST_EMB_PATH = \"/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ---- Embedding assets ----\n",
    "EMBED_A_TRAIN_IN = \"/kaggle/input/final-embeddings/train_embeddings_a.npy\"\n",
    "EMBED_B_TRAIN_IN = \"/kaggle/input/final-embeddings/train_embeddings_b.npy\"\n",
    "EMBED_A_TEST_IN  = \"/kaggle/input/final-embeddings/test_embeddings_a.npy\"\n",
    "EMBED_B_TEST_IN  = \"/kaggle/input/final-embeddings/test_embeddings_b.npy\"\n",
    "\n",
    "TRAIN_A_PATH = os.environ.get(\"TRAIN_A_PATH\", \"/kaggle/working/train_embeddings_a.npy\")\n",
    "TRAIN_B_PATH = os.environ.get(\"TRAIN_B_PATH\", \"/kaggle/working/train_embeddings_b.npy\")\n",
    "TEST_A_PATH  = os.environ.get(\"TEST_A_PATH\",  \"/kaggle/working/test_embeddings_a.npy\")\n",
    "TEST_B_PATH  = os.environ.get(\"TEST_B_PATH\",  \"/kaggle/working/test_embeddings_b.npy\")\n",
    "\n",
    "# LoRA backbone (must be a local folder in /kaggle/input)\n",
    "LORA_MODEL_PATH = os.environ.get(\"LORA_MODEL_PATH\", \"/kaggle/input/distil_bert/keras/distil_bert_base_en_uncased/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) IMPORTS & SEEDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:51.060948Z",
     "iopub.status.busy": "2025-11-05T22:27:51.060740Z",
     "iopub.status.idle": "2025-11-05T22:27:51.526502Z",
     "shell.execute_reply": "2025-11-05T22:27:51.525955Z",
     "shell.execute_reply.started": "2025-11-05T22:27:51.060933Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re, ast, random, warnings\n",
    "import numpy as np, pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) DATA LOADING & BASIC TEXT EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:51.528640Z",
     "iopub.status.busy": "2025-11-05T22:27:51.528263Z",
     "iopub.status.idle": "2025-11-05T22:27:57.062425Z",
     "shell.execute_reply": "2025-11-05T22:27:57.061821Z",
     "shell.execute_reply.started": "2025-11-05T22:27:51.528620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH).dropna()\n",
    "test_df = pd.read_csv(TEST_PATH).dropna()\n",
    "\n",
    "\n",
    "# Build 3-class target: 0=A, 1=B, 2=Tie\n",
    "y = np.select([\n",
    "train_df['winner_model_a'] == 1,\n",
    "train_df['winner_model_b'] == 1,\n",
    "train_df['winner_tie'] == 1\n",
    "], [0, 1, 2])\n",
    "classes = [0, 1, 2]\n",
    "\n",
    "\n",
    "# Text fields may be JSON-ish lists; normalize to strings\n",
    "def extract_text_from_field(text_field):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text_field)\n",
    "        if isinstance(parsed, list):\n",
    "            return ' '.join(map(str, parsed))\n",
    "        return str(parsed)\n",
    "    except Exception:\n",
    "        return str(text_field)\n",
    "\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n",
    "    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n",
    "    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n",
    "    df['text_a'] = df['prompt_text'] + ' [SEP] ' + df['response_a_text']\n",
    "    df['text_b'] = df['prompt_text'] + ' [SEP] ' + df['response_b_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) SHARED HELPERS (Folds, CV, Inference, Utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:57.063552Z",
     "iopub.status.busy": "2025-11-05T22:27:57.063265Z",
     "iopub.status.idle": "2025-11-05T22:27:57.086594Z",
     "shell.execute_reply": "2025-11-05T22:27:57.085841Z",
     "shell.execute_reply.started": "2025-11-05T22:27:57.063527Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_indices = [(tr, va) for tr, va in skf.split(np.zeros(len(y)), y)]\n",
    "\n",
    "def cv_calibrated_probs_numpy(X, y, method='sigmoid'):\n",
    "    \"\"\"Cross-validated calibrated probabilities for numpy arrays.\n",
    "       Returns OOF probs + list of fold models & scalers.\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y), f\"X rows {len(X)} must equal y {len(y)}\"\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models, scalers = [], []\n",
    "    for fold, (tr, va) in enumerate(fold_indices, 1):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(X[tr])\n",
    "        Xva_s = scaler.transform(X[va])\n",
    "\n",
    "        base = LogisticRegression(max_iter=2000, C=1.0, random_state=SEED)\n",
    "        clf  = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, y[tr])\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        print(f\"[CV {method}] fold {fold} logloss = {log_loss(y[va], proba, labels=classes):.5f}\")\n",
    "        models.append(clf); scalers.append(scaler)\n",
    "    print(f\"[CV {method}] OOF logloss = {log_loss(y, oof, labels=classes):.5f}\")\n",
    "    return oof, models, scalers\n",
    "\n",
    "def cv_calibrated_probs_df(X_df, y, method='sigmoid'):\n",
    "    \"\"\"Same as above but accepts a pandas DataFrame (for lexical features).\"\"\"\n",
    "    X = X_df.values.astype(float)\n",
    "    return cv_calibrated_probs_numpy(X, y, method=method)\n",
    "\n",
    "def predict_from_folds(models, scalers, X):\n",
    "    P = np.zeros((len(X), 3), dtype=float)\n",
    "    for clf, scaler in zip(models, scalers):\n",
    "        Xt = scaler.transform(X)\n",
    "        P += clf.predict_proba(Xt) / len(models)\n",
    "    return P\n",
    "\n",
    "# Simple 1-D weight search for 2-model blend\n",
    "def best_weight_for_blend(y, p1, p2, steps=101):\n",
    "    best_w, best_loss = 0.5, 1e9\n",
    "    for i in range(steps):\n",
    "        w = i/(steps-1)\n",
    "        blend = w*p1 + (1-w)*p2\n",
    "        loss  = log_loss(y, blend, labels=classes)\n",
    "        if loss < best_loss:\n",
    "            best_loss, best_w = loss, w\n",
    "    return best_w, best_loss\n",
    "\n",
    "# Optional temperature scaling (post-hoc smoothing on probs)\n",
    "def fit_temperature(p_oof, y, iters=50):\n",
    "    z = np.log(np.clip(p_oof, 1e-8, 1.0)).astype(float)\n",
    "    z = z - z.mean(axis=1, keepdims=True)\n",
    "    T, best = 1.0, 1e9\n",
    "    for _ in range(iters):\n",
    "        for t in np.linspace(max(0.5, T-0.5), T+0.5, 9):\n",
    "            q = np.exp(z/t); q /= q.sum(axis=1, keepdims=True)\n",
    "            ll = log_loss(y, q, labels=classes)\n",
    "            if ll < best:\n",
    "                best, T = ll, t\n",
    "    return T\n",
    "\n",
    "def apply_temperature(p, T):\n",
    "    z = np.log(np.clip(p, 1e-8, 1.0)).astype(float)\n",
    "    z = z - z.mean(axis=1, keepdims=True)\n",
    "    q = np.exp(z/T); q /= q.sum(axis=1, keepdims=True)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) LEXICAL FEATURES (bias-aware verbosity/structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:27:57.087681Z",
     "iopub.status.busy": "2025-11-05T22:27:57.087399Z",
     "iopub.status.idle": "2025-11-05T22:28:07.761420Z",
     "shell.execute_reply": "2025-11-05T22:28:07.760735Z",
     "shell.execute_reply.started": "2025-11-05T22:27:57.087655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_lex: (57477, 11)  X_lex_test: (3, 11)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(re.findall(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "def paragraph_count(t):\n",
    "    return t.count('\\n\\n') if isinstance(t, str) else 0\n",
    "\n",
    "def list_count(t):\n",
    "    return count_pattern(t, r\"^\\s*(?:[\\-\\*â€¢]\\s|\\d+\\.)\")\n",
    "\n",
    "def quote_count(t):\n",
    "    # '>' line quotes or **bold** emphasis as a proxy\n",
    "    return count_pattern(t, r\"(^>\\s|(?<!\\*)\\*\\*[^*]+\\*\\*)\")\n",
    "\n",
    "def sentence_count(t):\n",
    "    return count_pattern(t, r\"[.!?](?:\\s|$)\")\n",
    "\n",
    "def code_block_count(t):\n",
    "    return count_pattern(t, r\"```|`[^`]+`\")\n",
    "\n",
    "def heading_count(t):\n",
    "    return count_pattern(t, r\"^(?:#{1,6})\\s\")\n",
    "\n",
    "def word_count(t):\n",
    "    return len(t.split()) if isinstance(t, str) else 0\n",
    "\n",
    "# Build a rich lexical matrix for A/B and contrasts\n",
    "def build_lex_features(df, a_col='response_a_text', b_col='response_b_text'):\n",
    "    A = df[a_col].fillna('').astype(str)\n",
    "    B = df[b_col].fillna('').astype(str)\n",
    "    feats = {}\n",
    "    for name, series in [('a', A), ('b', B)]:\n",
    "        feats[f'len_{name}']   = series.map(len)\n",
    "        feats[f'wc_{name}']    = series.map(word_count)\n",
    "        feats[f'sent_{name}']  = series.map(sentence_count)\n",
    "        feats[f'para_{name}']  = series.map(paragraph_count)\n",
    "        feats[f'list_{name}']  = series.map(list_count)\n",
    "        feats[f'quote_{name}'] = series.map(quote_count)\n",
    "        feats[f'code_{name}']  = series.map(code_block_count)\n",
    "        feats[f'hdr_{name}']   = series.map(heading_count)\n",
    "    F = pd.DataFrame(feats).astype(float)\n",
    "    # Diffs & ratios\n",
    "    for base in ['len','wc','sent','para','list','quote','code','hdr']:\n",
    "        F[f'{base}_diff']  = F[f'{base}_a'] - F[f'{base}_b']\n",
    "    for base in ['len','wc','sent']:\n",
    "        F[f'{base}_ratio'] = (F[f'{base}_a'] + 1.0) / (F[f'{base}_b'] + 1.0)\n",
    "    # Densities per 100 words\n",
    "    eps = 1e-6\n",
    "    for base in ['sent','para','list','quote','code','hdr']:\n",
    "        F[f'{base}_per100w_a'] = 100.0 * F[f'{base}_a'] / (F['wc_a'] + eps)\n",
    "        F[f'{base}_per100w_b'] = 100.0 * F[f'{base}_b'] / (F['wc_b'] + eps)\n",
    "        F[f'{base}_per100w_diff'] = F[f'{base}_per100w_a'] - F[f'{base}_per100w_b']\n",
    "    F = F.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "    return F\n",
    "\n",
    "lex_cols_keep = [\n",
    "    # core diffs (matches the original minimal set)\n",
    "    'len_diff','wc_diff','sent_diff','para_diff','list_diff','quote_diff','code_diff','hdr_diff',\n",
    "    'len_ratio','wc_ratio','sent_ratio'\n",
    "]\n",
    "\n",
    "X_lex_train_df = build_lex_features(train_df)\n",
    "X_lex_test_df  = build_lex_features(test_df)\n",
    "# Keep a compact set to remain fully coherent with the original\n",
    "X_lex = X_lex_train_df[lex_cols_keep].copy()\n",
    "X_lex_test = X_lex_test_df[lex_cols_keep].copy()\n",
    "print(\"X_lex:\", X_lex.shape, \" X_lex_test:\", X_lex_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) LEXICAL MODEL (run if MODE in {lexical, ensemble})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:28:07.762374Z",
     "iopub.status.busy": "2025-11-05T22:28:07.762151Z",
     "iopub.status.idle": "2025-11-05T22:28:19.234243Z",
     "shell.execute_reply": "2025-11-05T22:28:19.232357Z",
     "shell.execute_reply.started": "2025-11-05T22:28:07.762356Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV sigmoid] fold 1 logloss = 1.07003\n",
      "[CV sigmoid] fold 2 logloss = 1.06940\n",
      "[CV sigmoid] fold 3 logloss = 1.07065\n",
      "[CV sigmoid] fold 4 logloss = 1.07088\n",
      "[CV sigmoid] fold 5 logloss = 1.06935\n",
      "[CV sigmoid] OOF logloss = 1.07006\n",
      "[CV isotonic] fold 1 logloss = 1.06102\n",
      "[CV isotonic] fold 2 logloss = 1.06256\n",
      "[CV isotonic] fold 3 logloss = 1.06173\n",
      "[CV isotonic] fold 4 logloss = 1.06018\n",
      "[CV isotonic] fold 5 logloss = 1.06073\n",
      "[CV isotonic] OOF logloss = 1.06124\n"
     ]
    }
   ],
   "source": [
    "lex_best_name = None\n",
    "lex_best_oof = None\n",
    "lex_best_models = []\n",
    "lex_best_scalers = []\n",
    "\n",
    "if MODE in {\"lexical\", \"ensemble\"}:\n",
    "    oof_lex_sig, lex_sig_models, lex_sig_scalers = cv_calibrated_probs_df(X_lex, y, method='sigmoid')\n",
    "    oof_lex_iso, lex_iso_models, lex_iso_scalers = cv_calibrated_probs_df(X_lex, y, method='isotonic')\n",
    "\n",
    "    if log_loss(y, oof_lex_iso, labels=classes) < log_loss(y, oof_lex_sig, labels=classes):\n",
    "        lex_best_name   = 'isotonic'\n",
    "        lex_best_oof    = oof_lex_iso\n",
    "        lex_best_models = lex_iso_models\n",
    "        lex_best_scalers= lex_iso_scalers\n",
    "    else:\n",
    "        lex_best_name   = 'sigmoid'\n",
    "        lex_best_oof    = oof_lex_sig\n",
    "        lex_best_models = lex_sig_models\n",
    "        lex_best_scalers= lex_sig_scalers\n",
    "\n",
    "    # TEST inference\n",
    "    Xlt = X_lex_test.values.astype(float)\n",
    "    lex_proba_test = predict_from_folds(lex_best_models, lex_best_scalers, Xlt)\n",
    "    sub_lex = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': lex_proba_test[:,0],\n",
    "    'winner_model_b': lex_proba_test[:,1],\n",
    "    'winner_tie':     lex_proba_test[:,2],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) EMBEDDINGS (run if MODE in {embeddings, ensemble})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:28:19.235169Z",
     "iopub.status.busy": "2025-11-05T22:28:19.234936Z",
     "iopub.status.idle": "2025-11-05T22:37:32.745007Z",
     "shell.execute_reply": "2025-11-05T22:37:32.744060Z",
     "shell.execute_reply.started": "2025-11-05T22:28:19.235150Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 22:28:24.527325: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762381704.551609      92 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762381704.558740      92 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loading embeddings from INPUT DATASET paths...\n",
      "Embedding shapes: (57477, 768) (3, 768)\n",
      "[CV sigmoid] fold 1 logloss = 1.05724\n",
      "[CV sigmoid] fold 2 logloss = 1.06211\n",
      "[CV sigmoid] fold 3 logloss = 1.05590\n",
      "[CV sigmoid] fold 4 logloss = 1.05847\n",
      "[CV sigmoid] fold 5 logloss = 1.05856\n",
      "[CV sigmoid] OOF logloss = 1.05846\n",
      "[CV isotonic] fold 1 logloss = 1.05629\n",
      "[CV isotonic] fold 2 logloss = 1.06120\n",
      "[CV isotonic] fold 3 logloss = 1.05486\n",
      "[CV isotonic] fold 4 logloss = 1.05782\n",
      "[CV isotonic] fold 5 logloss = 1.06042\n",
      "[CV isotonic] OOF logloss = 1.05812\n",
      "\n",
      "âœ… Best embedding calibration: isotonic\n"
     ]
    }
   ],
   "source": [
    "# --- SINGLE embeddings block (keep only this one) ---\n",
    "X_emb_train = None\n",
    "X_emb_test  = None\n",
    "emb_best_name = None\n",
    "emb_best_oof = None\n",
    "emb_best_models = []\n",
    "emb_best_scalers = []\n",
    "\n",
    "if MODE in {\"embeddings\", \"ensemble\"}:\n",
    "    import ast, os, torch\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    def _all_exist(paths):\n",
    "        return all(isinstance(p, str) and p != \"NONE\" and os.path.exists(p) for p in paths)\n",
    "\n",
    "    # 1) Try input-dataset .npy\n",
    "    if _all_exist([EMBED_A_TRAIN_IN, EMBED_B_TRAIN_IN, EMBED_A_TEST_IN, EMBED_B_TEST_IN]):\n",
    "        print(\"âœ… Loading embeddings from INPUT DATASET paths...\")\n",
    "        train_a = np.load(EMBED_A_TRAIN_IN); train_b = np.load(EMBED_B_TRAIN_IN)\n",
    "        test_a  = np.load(EMBED_A_TEST_IN);  test_b  = np.load(EMBED_B_TEST_IN)\n",
    "        # also save to working dir so future runs detect them\n",
    "        np.save(TRAIN_A_PATH, train_a); np.save(TRAIN_B_PATH, train_b)\n",
    "        np.save(TEST_A_PATH,  test_a);  np.save(TEST_B_PATH,  test_b)\n",
    "\n",
    "    # 2) Else working-dir cache\n",
    "    elif _all_exist([TRAIN_A_PATH, TRAIN_B_PATH, TEST_A_PATH, TEST_B_PATH]):\n",
    "        print(\"âœ… Loading embeddings from WORKING DIR cache...\")\n",
    "        train_a = np.load(TRAIN_A_PATH); train_b = np.load(TRAIN_B_PATH)\n",
    "        test_a  = np.load(TEST_A_PATH);  test_b  = np.load(TEST_B_PATH)\n",
    "\n",
    "    # 3) Else compute (and save to working dir)\n",
    "    else:\n",
    "        print(\"âš ï¸ No saved embeddings found â†’ computing now...\")\n",
    "\n",
    "        def extract_text_from_field(v):\n",
    "            if v is None or (isinstance(v, float) and pd.isna(v)): return \"\"\n",
    "            try:\n",
    "                parsed = ast.literal_eval(str(v))\n",
    "                if isinstance(parsed, list): return \" \".join(str(x) for x in parsed)\n",
    "                return str(parsed)\n",
    "            except Exception:\n",
    "                return str(v)\n",
    "\n",
    "        for df in (train_df, test_df):\n",
    "            df[\"prompt_text\"]     = df[\"prompt\"].apply(extract_text_from_field)\n",
    "            df[\"response_a_text\"] = df[\"response_a\"].apply(extract_text_from_field)\n",
    "            df[\"response_b_text\"] = df[\"response_b\"].apply(extract_text_from_field)\n",
    "            df[\"text_a\"] = df[\"prompt_text\"] + \" [SEP] \" + df[\"response_a_text\"]\n",
    "            df[\"text_b\"] = df[\"prompt_text\"] + \" [SEP] \" + df[\"response_b_text\"]\n",
    "\n",
    "        train_text_a = train_df[\"text_a\"].tolist()\n",
    "        train_text_b = train_df[\"text_b\"].tolist()\n",
    "        test_text_a  = test_df[\"text_a\"].tolist()\n",
    "        test_text_b  = test_df[\"text_b\"].tolist()\n",
    "\n",
    "        print(\"\\nLoading SentenceTransformer model from:\", ST_EMB_PATH)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = SentenceTransformer(ST_EMB_PATH, device=device)\n",
    "        print(f\"â†’ Running embeddings on device: {device}\")\n",
    "\n",
    "        slow_tok = AutoTokenizer.from_pretrained(ST_EMB_PATH, use_fast=False, local_files_only=True)\n",
    "        model._first_module().tokenizer = slow_tok\n",
    "        model.max_seq_length = 512\n",
    "\n",
    "        BATCH_SIZE = 32\n",
    "        print(\"\\nEncoding training embeddings A...\")\n",
    "        train_a = model.encode(train_text_a, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True)\n",
    "        print(\"\\nEncoding training embeddings B...\")\n",
    "        train_b = model.encode(train_text_b, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True)\n",
    "        print(\"\\nEncoding test embeddings A...\")\n",
    "        test_a = model.encode(test_text_a, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True)\n",
    "        print(\"\\nEncoding test embeddings B...\")\n",
    "        test_b = model.encode(test_text_b, batch_size=BATCH_SIZE, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "        np.save(TRAIN_A_PATH, train_a); np.save(TRAIN_B_PATH, train_b)\n",
    "        np.save(TEST_A_PATH,  test_a);  np.save(TEST_B_PATH,  test_b)\n",
    "        print(f\"ðŸ’¾ Saved embeddings to:\\n  {TRAIN_A_PATH}\\n  {TRAIN_B_PATH}\\n  {TEST_A_PATH}\\n  {TEST_B_PATH}\")\n",
    "\n",
    "    # Build, calibrate, submit\n",
    "    X_emb_train = np.concatenate([train_a, train_b], axis=1)\n",
    "    X_emb_test  = np.concatenate([test_a,  test_b],  axis=1)\n",
    "    print(\"Embedding shapes:\", X_emb_train.shape, X_emb_test.shape)\n",
    "\n",
    "    oof_emb_sig, emb_sig_models, emb_sig_scalers = cv_calibrated_probs_numpy(X_emb_train, y, method=\"sigmoid\")\n",
    "    oof_emb_iso, emb_iso_models, emb_iso_scalers = cv_calibrated_probs_numpy(X_emb_train, y, method=\"isotonic\")\n",
    "\n",
    "    if log_loss(y, oof_emb_iso, labels=classes) < log_loss(y, oof_emb_sig, labels=classes):\n",
    "        emb_best_name, emb_best_oof = \"isotonic\", oof_emb_iso\n",
    "        emb_best_models, emb_best_scalers = emb_iso_models, emb_iso_scalers\n",
    "    else:\n",
    "        emb_best_name, emb_best_oof = \"sigmoid\", oof_emb_sig\n",
    "        emb_best_models, emb_best_scalers = emb_sig_models, emb_sig_scalers\n",
    "\n",
    "    print(f\"\\nâœ… Best embedding calibration: {emb_best_name}\")\n",
    "\n",
    "    emb_proba_test = predict_from_folds(emb_best_models, emb_best_scalers, X_emb_test)\n",
    "else:\n",
    "    print(\"MODE does not include embeddings â†’ skipping.\")\n",
    "    emb_proba_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:37:32.746533Z",
     "iopub.status.busy": "2025-11-05T22:37:32.745917Z",
     "iopub.status.idle": "2025-11-05T22:37:32.761309Z",
     "shell.execute_reply": "2025-11-05T22:37:32.760572Z",
     "shell.execute_reply.started": "2025-11-05T22:37:32.746512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7) EMBEDDINGS_LORA (run if MODE in {embeddings_lora, ensemble})\n",
    "lora_test_proba = None\n",
    "lora_oof = None\n",
    "\n",
    "if MODE == \"embeddings_lora\":\n",
    "    import os, torch\n",
    "    from datasets import Dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    if not os.path.isdir(LORA_MODEL_PATH):\n",
    "        raise RuntimeError(\n",
    "            f\"LoRA MODE selected but local model folder not found at '{LORA_MODEL_PATH}'. \"\n",
    "            \"Attach a model to the notebook and set LORA_MODEL_PATH accordingly.\"\n",
    "        )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Build compact triplets from normalized text columns (already created earlier)\n",
    "    def build_input(df):\n",
    "        # short tags reduce sequence length vs full [SEP] variants\n",
    "        return (\n",
    "            '[PROMPT] ' + df['prompt_text'] +\n",
    "            ' [A] ' + df['response_a_text'] +\n",
    "            ' [B] ' + df['response_b_text']\n",
    "        )\n",
    "\n",
    "    train_inputs = build_input(train_df)\n",
    "    test_inputs  = build_input(test_df)\n",
    "\n",
    "    # Tokenizer & datasets (offline)\n",
    "    # local_files_only=True + HF offline env ensures no internet hit\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_PATH, use_fast=True, local_files_only=True)\n",
    "\n",
    "    MAX_LEN = 256  # faster, usually no accuracy drop for this task\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=MAX_LEN)\n",
    "\n",
    "    ds_train = Dataset.from_pandas(pd.DataFrame({'text': train_inputs, 'label': y}))\n",
    "    ds_test  = Dataset.from_pandas(pd.DataFrame({'text': test_inputs}))\n",
    "    tokenized_train = ds_train.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "    tokenized_test  = ds_test.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "    # Small train/val split to produce pseudo-OOF for ensembling/calibration\n",
    "    tr_idx, va_idx = train_test_split(np.arange(len(ds_train)), test_size=0.15, random_state=SEED, stratify=y)\n",
    "    ds_tr = tokenized_train.select(tr_idx.tolist())\n",
    "    ds_va = tokenized_train.select(va_idx.tolist())\n",
    "\n",
    "    # Model (offline) + LoRA adapters\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        LORA_MODEL_PATH, num_labels=3, torch_dtype=torch_dtype, local_files_only=True\n",
    "    )\n",
    "\n",
    "    # Choose reasonable target modules for LoRA depending on backbone\n",
    "    # DistilBERT uses q_lin/v_lin; BERT/Roberta use query/key/value; DeBERTa uses *_proj\n",
    "    all_module_names = [n for n,_ in model.named_modules()]\n",
    "    if any(\"q_lin\" in n for n in all_module_names) or any(\"v_lin\" in n for n in all_module_names):\n",
    "        target_modules = [\"q_lin\",\"v_lin\"]\n",
    "    elif any(\".query\" in n for n in all_module_names):\n",
    "        target_modules = [\"query\",\"key\",\"value\",\"dense\"]\n",
    "    elif any(\"q_proj\" in n for n in all_module_names):\n",
    "        target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"dense\"]\n",
    "    else:\n",
    "        # generic fallback\n",
    "        target_modules = [\"query\",\"key\",\"value\",\"dense\",\"q_proj\",\"k_proj\",\"v_proj\",\"q_lin\",\"v_lin\"]\n",
    "\n",
    "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, task_type='SEQ_CLS', target_modules=target_modules)\n",
    "    model = get_peft_model(model, peft_config).to(device)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        return {'log_loss': log_loss(labels, probs, labels=classes)}\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='/kaggle/working/out_lora',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,                         # keep light for Kaggle\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4,\n",
    "        evaluation_strategy='steps', eval_steps=200,\n",
    "        logging_steps=100,\n",
    "        save_strategy='no',\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "\n",
    "    # Temperature fit on val for mild calibration (offline)\n",
    "    with torch.no_grad():\n",
    "        va_logits = torch.tensor(trainer.predict(ds_va).predictions, device=device)\n",
    "        va_labels = torch.tensor(y[va_idx], device=device)\n",
    "\n",
    "    temperature = torch.nn.Parameter(torch.ones((), device=device))\n",
    "    opt = torch.optim.LBFGS([temperature], lr=0.1, max_iter=50)\n",
    "    def nll_with_temperature():\n",
    "        opt.zero_grad()\n",
    "        scaled = va_logits / temperature.clamp_min(1e-3)\n",
    "        loss = torch.nn.functional.cross_entropy(scaled, va_labels)\n",
    "        loss.backward(); return loss\n",
    "    opt.step(nll_with_temperature)\n",
    "    T = float(temperature.detach().cpu().numpy())\n",
    "    print(f'[LoRA] fitted temperature: T={T:.3f}')\n",
    "\n",
    "    # Build pseudo-OOF (for ensemble weight search) and test probabilities\n",
    "    with torch.no_grad():\n",
    "        tr_logits = torch.tensor(trainer.predict(ds_tr).predictions, device=device)\n",
    "        tr_probs  = torch.softmax(tr_logits / T, dim=1).cpu().numpy()\n",
    "        va_probs  = torch.softmax(va_logits / T, dim=1).cpu().numpy()\n",
    "    lora_oof = np.zeros((len(train_df), 3), dtype=float)\n",
    "    lora_oof[tr_idx] = tr_probs; lora_oof[va_idx] = va_probs\n",
    "    print('[LoRA] pseudo-OOF log_loss:', log_loss(y, lora_oof, labels=classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_logits = torch.tensor(trainer.predict(tokenized_test).predictions, device=device)\n",
    "        lora_test_proba = torch.softmax(test_logits / T, dim=1).cpu().numpy()\n",
    "else:\n",
    "    print(\"MODE does not include embeddings_lora â†’ skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) ENSEMBLE (only if MODE==\"ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:37:32.763205Z",
     "iopub.status.busy": "2025-11-05T22:37:32.762954Z",
     "iopub.status.idle": "2025-11-05T22:37:37.955527Z",
     "shell.execute_reply": "2025-11-05T22:37:37.954512Z",
     "shell.execute_reply.started": "2025-11-05T22:37:32.763190Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best blend weight for lex: 0.45 (OOF logloss=1.05122)\n",
      "Applied temperature scaling: T=0.688\n"
     ]
    }
   ],
   "source": [
    "if MODE == \"ensemble\":\n",
    "    sources = []\n",
    "    tests   = []\n",
    "    if lex_best_oof is not None:\n",
    "        sources.append((\"lex\", lex_best_oof))\n",
    "        tests.append((\"lex\", predict_from_folds(lex_best_models, lex_best_scalers, X_lex_test.values.astype(float))))\n",
    "    if emb_best_oof is not None and X_emb_test is not None:\n",
    "        sources.append((\"emb\", emb_best_oof))\n",
    "        tests.append((\"emb\", predict_from_folds(emb_best_models, emb_best_scalers, X_emb_test)))\n",
    "    if lora_oof is not None and lora_test_proba is not None:\n",
    "        sources.append((\"lora\", lora_oof))\n",
    "        tests.append((\"lora\", lora_test_proba))\n",
    "\n",
    "    if len(sources) == 0:\n",
    "        raise RuntimeError(\"No component models available to ensemble. Run MODE='lexical' or 'embeddings' first.\")\n",
    "\n",
    "    # Two-source blend -> 1D grid search; 3+ sources -> simple stacker\n",
    "    if len(sources) == 1:\n",
    "        name, oof = sources[0]\n",
    "        test = dict(tests)[name]\n",
    "        final_test = test\n",
    "        print(f\"Only one source ({name}); using it as ensemble output.\")\n",
    "    elif len(sources) == 2:\n",
    "        (n1, o1), (n2, o2) = sources\n",
    "        (t1_name, t1), (t2_name, t2) = tests\n",
    "        assert n1==t1_name and n2==t2_name\n",
    "        w, best = best_weight_for_blend(y, o1, o2)\n",
    "        print(f\"Best blend weight for {n1}: {w:.2f} (OOF logloss={best:.5f})\")\n",
    "        final_test = w*t1 + (1-w)*t2\n",
    "    else:\n",
    "        # Tiny stacker (multinomial LR) on concatenated OOF probs\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        names = [n for n,_ in sources]\n",
    "        OOF   = np.hstack([p for _,p in sources])\n",
    "        meta  = LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=SEED)\n",
    "        meta.fit(OOF, y)\n",
    "        TEST  = np.hstack([dict(tests)[n] for n in names])\n",
    "        final_test = meta.predict_proba(TEST)\n",
    "        print(\"Stacker trained over sources:\", names)\n",
    "\n",
    "    # Optional: temperature-scale the ensemble using best available OOF proxy (concat if needed)\n",
    "    try:\n",
    "        if len(sources) >= 1:\n",
    "            if len(sources) == 1:\n",
    "                oof_ref = sources[0][1]\n",
    "            else:\n",
    "                # average OOF as a rough proxy for calibration\n",
    "                oof_ref = np.mean([p for _,p in sources], axis=0)\n",
    "            T = fit_temperature(oof_ref, y)\n",
    "            final_test = apply_temperature(final_test, T)\n",
    "            print(f\"Applied temperature scaling: T={T:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Temp scaling skipped:\", e)\n",
    "\n",
    "    sub_blend = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': final_test[:,0],\n",
    "    'winner_model_b': final_test[:,1],\n",
    "    'winner_tie':     final_test[:,2],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:37:37.956678Z",
     "iopub.status.busy": "2025-11-05T22:37:37.956419Z",
     "iopub.status.idle": "2025-11-05T22:37:37.965360Z",
     "shell.execute_reply": "2025-11-05T22:37:37.964714Z",
     "shell.execute_reply.started": "2025-11-05T22:37:37.956658Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Wrote submission.csv using MODE=\"ensemble\"\n"
     ]
    }
   ],
   "source": [
    "chosen = None\n",
    "\n",
    "if MODE == \"lexical\":\n",
    "    assert 'lex_proba_test' in globals(), \"Run lexical block first.\"\n",
    "    chosen = lex_proba_test\n",
    "\n",
    "elif MODE == \"embeddings\":\n",
    "    assert 'emb_proba_test' in globals(), \"Run embeddings block first.\"\n",
    "    chosen = emb_proba_test\n",
    "\n",
    "elif MODE == \"embeddings_lora\":\n",
    "    assert 'lora_test_proba' in globals(), \"Run embeddings_lora block first.\"\n",
    "    chosen = lora_test_proba\n",
    "\n",
    "elif MODE == \"ensemble\":\n",
    "    assert 'final_test' in globals(), \"Run ensemble block to produce final_test.\"\n",
    "    chosen = final_test\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODE: {MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF predictions to numpy files\n",
    "np.save('/kaggle/working/lex_best_oof.npy', lex_best_oof)\n",
    "np.save('/kaggle/working/emb_best_oof.npy', emb_best_oof)\n",
    "np.save('/kaggle/working/y_true.npy', y)\n",
    "\n",
    "\n",
    "# Save X_lex for structural feature analysis\n",
    "X_lex.to_csv('/kaggle/working/X_lex_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9809560,
     "isSourceIdPinned": false,
     "sourceId": 86518,
     "sourceType": "competition"
    },
    {
     "datasetId": 8660591,
     "sourceId": 13626489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8661356,
     "sourceId": 13627570,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 95613,
     "modelInstanceId": 70561,
     "sourceId": 84013,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 2821,
     "modelInstanceId": 4689,
     "sourceId": 205028,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 491206,
     "modelInstanceId": 475312,
     "sourceId": 630753,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
