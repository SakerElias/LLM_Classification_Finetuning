{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23592e9",
   "metadata": {},
   "source": [
    "# Step 3 — Extensions: Bias features, Calibration, Ensembling, and LoRA fine-tuning\n",
    "\n",
    "This notebook implements the full Step 3 pipeline without touching prior notebooks:\n",
    "\n",
    "- Bias-aware lexical features (verbosity and structure)\n",
    "- Calibrated classifiers (sigmoid and isotonic)\n",
    "- Embeddings-based model (reusing precomputed .npy when available)\n",
    "- Simple ensembling via OOF-weight search\n",
    "- Optional lightweight LoRA fine-tuning with temperature scaling\n",
    "\n",
    "Outputs: submission CSVs for each component and a blended ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61179c45",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\llm-classification-finetuning\\\\train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m TRAIN_PATH \u001b[38;5;241m=\u001b[39m DATA \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllm-classification-finetuning/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     18\u001b[0m TEST_PATH \u001b[38;5;241m=\u001b[39m DATA \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 21\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m test_df  \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(TEST_PATH)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Build 3-class target: 0=A, 1=B, 2=Tie\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Elias\\OneDrive\\Bureau\\ecole\\Chung Ang\\MLP\\LLM_Classification_Finetuning\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elias\\OneDrive\\Bureau\\ecole\\Chung Ang\\MLP\\LLM_Classification_Finetuning\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Elias\\OneDrive\\Bureau\\ecole\\Chung Ang\\MLP\\LLM_Classification_Finetuning\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Elias\\OneDrive\\Bureau\\ecole\\Chung Ang\\MLP\\LLM_Classification_Finetuning\\.venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Elias\\OneDrive\\Bureau\\ecole\\Chung Ang\\MLP\\LLM_Classification_Finetuning\\.venv\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\llm-classification-finetuning\\\\train.csv'"
     ]
    }
   ],
   "source": [
    "# Imports & setup\n",
    "import re, ast, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA = Path('../data')\n",
    "TRAIN_PATH = DATA / 'train.csv'\n",
    "TEST_PATH = DATA / 'test.csv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Build 3-class target: 0=A, 1=B, 2=Tie\n",
    "y = np.select([train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1], [0,1,2])\n",
    "train_df['target'] = y\n",
    "classes = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee06ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction utilities\n",
    "def extract_text_from_field(text_field):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text_field)\n",
    "        return ' '.join(parsed) if isinstance(parsed, list) else str(parsed)\n",
    "    except Exception:\n",
    "        return str(text_field)\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n",
    "    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n",
    "    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n",
    "    df['text_a'] = df['prompt_text'] + ' [SEP] ' + df['response_a_text']\n",
    "    df['text_b'] = df['prompt_text'] + ' [SEP] ' + df['response_b_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a26d75",
   "metadata": {},
   "source": [
    "## Bias-aware and structural lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57487ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 11), (3, 11))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structural counters\n",
    "\n",
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "\n",
    "def paragraph_count(t):\n",
    "    return t.count('\\n\\n') if isinstance(t, str) else 0\n",
    "\n",
    "\n",
    "def list_count(t):\n",
    "    return count_pattern(t, r'(^\\s*[\\-\\*•]\\s|\\d+\\.)')\n",
    "\n",
    "\n",
    "def quote_count(t):\n",
    "    return count_pattern(t, r'>|\\*\\*')\n",
    "\n",
    "\n",
    "def sentence_count(t):\n",
    "    return count_pattern(t, r'[.!?](\\s|$)')\n",
    "\n",
    "\n",
    "def code_block_count(t):\n",
    "    return count_pattern(t, r'```|`[^`]+`')\n",
    "\n",
    "\n",
    "def heading_count(t):\n",
    "    return count_pattern(t, r'^(#|##|###|####|#####|######)\\s')\n",
    "\n",
    "\n",
    "def word_count(t):\n",
    "    return len(t.split()) if isinstance(t, str) else 0\n",
    "\n",
    "# Compute per-side features using explicit suffix mapping ('a' / 'b')\n",
    "for df in (train_df, test_df):\n",
    "    for col, suffix in [('response_a_text', 'a'), ('response_b_text', 'b')]:\n",
    "        df[f'len_{suffix}']   = df[col].astype(str).apply(len)\n",
    "        df[f'wc_{suffix}']    = df[col].astype(str).apply(word_count)\n",
    "        df[f'sent_{suffix}']  = df[col].apply(sentence_count)\n",
    "        df[f'para_{suffix}']  = df[col].apply(paragraph_count)\n",
    "        df[f'list_{suffix}']  = df[col].apply(list_count)\n",
    "        df[f'quote_{suffix}'] = df[col].apply(quote_count)\n",
    "        df[f'code_{suffix}']  = df[col].apply(code_block_count)\n",
    "        df[f'hdr_{suffix}']   = df[col].apply(heading_count)\n",
    "\n",
    "    # Diffs (A - B) — captures verbosity and structure bias\n",
    "    for base in ['len','wc','sent','para','list','quote','code','hdr']:\n",
    "        df[f'{base}_diff'] = df[f'{base}_a'] - df[f'{base}_b']\n",
    "\n",
    "    # Ratios (A / (B+1)) to capture scale-invariant verbosity bias\n",
    "    for base in ['len','wc','sent']:\n",
    "        df[f'{base}_ratio'] = df[f'{base}_a'] / (df[f'{base}_b'] + 1.0)\n",
    "\n",
    "LEX_FEATURES = [\n",
    "    'len_diff','wc_diff','sent_diff','para_diff','list_diff','quote_diff','code_diff','hdr_diff',\n",
    "    'len_ratio','wc_ratio','sent_ratio'\n",
    "]\n",
    "X_lex = train_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex_test = test_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex.shape, X_lex_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1f8a7",
   "metadata": {},
   "source": [
    "## Calibrated lexical model (sigmoid and isotonic) with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9917e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lex sigmoid] Fold 1: log_loss=1.06936\n",
      "[Lex sigmoid] Fold 2: log_loss=1.06919\n",
      "[Lex sigmoid] Fold 3: log_loss=1.07030\n",
      "[Lex sigmoid] Fold 4: log_loss=1.07064\n",
      "[Lex sigmoid] Fold 3: log_loss=1.07030\n",
      "[Lex sigmoid] Fold 4: log_loss=1.07064\n",
      "[Lex sigmoid] Fold 5: log_loss=1.06862\n",
      "[Lex sigmoid] OOF log_loss: 1.06962\n",
      "[Lex isotonic] Fold 1: log_loss=1.06078\n",
      "[Lex sigmoid] Fold 5: log_loss=1.06862\n",
      "[Lex sigmoid] OOF log_loss: 1.06962\n",
      "[Lex isotonic] Fold 1: log_loss=1.06078\n",
      "[Lex isotonic] Fold 2: log_loss=1.06198\n",
      "[Lex isotonic] Fold 3: log_loss=1.06130\n",
      "[Lex isotonic] Fold 2: log_loss=1.06198\n",
      "[Lex isotonic] Fold 3: log_loss=1.06130\n",
      "[Lex isotonic] Fold 4: log_loss=1.05976\n",
      "[Lex isotonic] Fold 5: log_loss=1.05932\n",
      "[Lex isotonic] OOF log_loss: 1.06063\n",
      "Best lexical calibration: isotonic OOF log_loss= 1.060625553766379\n",
      "[Lex isotonic] Fold 4: log_loss=1.05976\n",
      "[Lex isotonic] Fold 5: log_loss=1.05932\n",
      "[Lex isotonic] OOF log_loss: 1.06063\n",
      "Best lexical calibration: isotonic OOF log_loss= 1.060625553766379\n"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs(X, y, base_model=None, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "    if base_model is None:\n",
    "        base_model = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models = []\n",
    "    scalers = []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        clf = CalibratedClassifierCV(estimator=base_model, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Lex {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf)\n",
    "        scalers.append(scaler)\n",
    "    print(f'[Lex {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_lex_sigmoid, lex_sigmoid_models, lex_sigmoid_scalers = cv_calibrated_probs(X_lex, y, method='sigmoid')\n",
    "oof_lex_isotonic, lex_isotonic_models, lex_isotonic_scalers = cv_calibrated_probs(X_lex, y, method='isotonic')\n",
    "\n",
    "# Choose the better calibration by OOF loss\n",
    "lex_oof_list = [('sigmoid', oof_lex_sigmoid), ('isotonic', oof_lex_isotonic)]\n",
    "lex_best_name, lex_best_oof = min(lex_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best lexical calibration:', lex_best_name, 'OOF log_loss=', log_loss(y, lex_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6260c41",
   "metadata": {},
   "source": [
    "## Embedding features (reuse precomputed .npy if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 768), (3, 768))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_A_TRAIN = Path('train_embeddings_a.npy')\n",
    "EMBED_B_TRAIN = Path('train_embeddings_b.npy')\n",
    "EMBED_A_TEST  = Path('test_embeddings_a.npy')\n",
    "EMBED_B_TEST  = Path('test_embeddings_b.npy')\n",
    "\n",
    "def ensure_embeddings(train_df, test_df, batch_size=32, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_a = model.encode(train_df['text_a'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    train_b = model.encode(train_df['text_b'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_a  = model.encode(test_df['text_a'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_b  = model.encode(test_df['text_b'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    np.save(EMBED_A_TRAIN, train_a); np.save(EMBED_B_TRAIN, train_b)\n",
    "    np.save(EMBED_A_TEST,  test_a);  np.save(EMBED_B_TEST,  test_b)\n",
    "    return train_a, train_b, test_a, test_b\n",
    "\n",
    "if EMBED_A_TRAIN.exists() and EMBED_B_TRAIN.exists() and EMBED_A_TEST.exists() and EMBED_B_TEST.exists():\n",
    "    train_a = np.load(EMBED_A_TRAIN)\n",
    "    train_b = np.load(EMBED_B_TRAIN)\n",
    "    test_a  = np.load(EMBED_A_TEST)\n",
    "    test_b  = np.load(EMBED_B_TEST)\n",
    "else:\n",
    "    train_a, train_b, test_a, test_b = ensure_embeddings(train_df, test_df)\n",
    "\n",
    "X_emb = np.concatenate([train_a, train_b], axis=1)\n",
    "X_emb_test = np.concatenate([test_a, test_b], axis=1)\n",
    "X_emb.shape, X_emb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18348",
   "metadata": {},
   "source": [
    "## Calibrated embeddings model with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93cef7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[Emb \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] OOF log_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_loss(y,\u001b[38;5;250m \u001b[39moof,\u001b[38;5;250m \u001b[39mlabels=classes)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m oof, models, scalers\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m oof_emb_sigmoid, emb_sigmoid_models, emb_sigmoid_scalers = \u001b[43mcv_calibrated_probs_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msigmoid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m oof_emb_isotonic, emb_isotonic_models, emb_isotonic_scalers = cv_calibrated_probs_numpy(X_emb, y, method=\u001b[33m'\u001b[39m\u001b[33misotonic\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m emb_oof_list = [(\u001b[33m'\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m'\u001b[39m, oof_emb_sigmoid), (\u001b[33m'\u001b[39m\u001b[33misotonic\u001b[39m\u001b[33m'\u001b[39m, oof_emb_isotonic)]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mcv_calibrated_probs_numpy\u001b[39m\u001b[34m(X, y, method, n_splits, seed)\u001b[39m\n\u001b[32m     10\u001b[39m Xva_s = scaler.transform(Xva)\n\u001b[32m     11\u001b[39m base = LogisticRegression(max_iter=\u001b[32m2000\u001b[39m, C=\u001b[32m1.0\u001b[39m, random_state=seed)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m clf = \u001b[43mCalibratedClassifierCV\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m clf.fit(Xtr_s, ytr)\n\u001b[32m     14\u001b[39m proba = clf.predict_proba(Xva_s)\n",
      "\u001b[31mTypeError\u001b[39m: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs_numpy(X, y, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models, scalers = [], []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X[tr], X[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        base = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "        clf = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Emb {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf); scalers.append(scaler)\n",
    "    print(f'[Emb {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_emb_sigmoid, emb_sigmoid_models, emb_sigmoid_scalers = cv_calibrated_probs_numpy(X_emb, y, method='sigmoid')\n",
    "oof_emb_isotonic, emb_isotonic_models, emb_isotonic_scalers = cv_calibrated_probs_numpy(X_emb, y, method='isotonic')\n",
    "\n",
    "emb_oof_list = [('sigmoid', oof_emb_sigmoid), ('isotonic', oof_emb_isotonic)]\n",
    "emb_best_name, emb_best_oof = min(emb_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best embedding calibration:', emb_best_name, 'OOF log_loss=', log_loss(y, emb_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c54b32",
   "metadata": {},
   "source": [
    "## Simple ensemble (OOF-weight search on lexical + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search weights w in [0..1] for p = w*p_lex + (1-w)*p_emb minimizing OOF log_loss\n",
    "def best_weight_for_blend(y, p_lex, p_emb, steps=101):\n",
    "    best_w, best_loss = 0.5, 1e9\n",
    "    for i in range(steps):\n",
    "        w = i/(steps-1)\n",
    "        blend = w*p_lex + (1-w)*p_emb\n",
    "        loss = log_loss(y, blend, labels=classes)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss; best_w = w\n",
    "    return best_w, best_loss\n",
    "\n",
    "w_blend, loss_blend = best_weight_for_blend(y, lex_best_oof, emb_best_oof)\n",
    "print(f'Ensemble best weight (lexical): {w_blend:.2f}, OOF log_loss: {loss_blend:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e491d",
   "metadata": {},
   "source": [
    "## Optional: Lightweight LoRA fine-tuning with temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LORA = False  # set True to train (can be slow)\n",
    "LORA_MODEL_NAME = 'distilbert-base-uncased'  # small and fast; change to 'microsoft/deberta-v3-small' if desired\n",
    "\n",
    "lora_oof = None\n",
    "lora_test_proba = None\n",
    "\n",
    "if RUN_LORA:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from datasets import Dataset\n",
    "\n",
    "    # Build text inputs for classification (prompt + A + B)\n",
    "    def build_input(df):\n",
    "        return (\n",
    "            '[PROMPT] ' + df['prompt_text'] +\n",
    "            ' [A] ' + df['response_a_text'] +\n",
    "            ' [B] ' + df['response_b_text']\n",
    "        )\n",
    "\n",
    "    train_inputs = build_input(train_df)\n",
    "    test_inputs  = build_input(test_df)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_NAME)\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    ds_train = Dataset.from_pandas(pd.DataFrame({'text': train_inputs, 'label': y}))\n",
    "    ds_test  = Dataset.from_pandas(pd.DataFrame({'text': test_inputs}))\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL_NAME, num_labels=3)\n",
    "\n",
    "    # Auto-detect common attention module names for LoRA targets\n",
    "    target_keywords = ['q_proj','v_proj','k_proj','query','key','value','q_lin','v_lin']\n",
    "    all_module_names = [n for n,_ in model.named_modules()]\n",
    "    target_modules = sorted({n.split('.')[-1] for n in all_module_names if any(k in n for k in target_keywords)})\n",
    "    if not target_modules:\n",
    "        # fallback for DistilBERT attention names\n",
    "        target_modules = ['q_lin','v_lin']\n",
    "\n",
    "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, task_type='SEQ_CLS', target_modules=target_modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    tokenized_train = ds_train.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "    tokenized_test  = ds_test.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        return {'log_loss': log_loss(labels, probs, labels=classes)}\n",
    "\n",
    "    # Simple split for evaluation and temperature fitting\n",
    "    tr_idx, va_idx = train_test_split(np.arange(len(ds_train)), test_size=0.15, random_state=RANDOM_STATE, stratify=y)\n",
    "    ds_tr = tokenized_train.select(tr_idx.tolist())\n",
    "    ds_va = tokenized_train.select(va_idx.tolist())\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='out_lora',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=200,\n",
    "        logging_steps=100,\n",
    "        save_strategy='no',\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "\n",
    "    # Temperature scaling on validation logits\n",
    "    with torch.no_grad():\n",
    "        va_logits = torch.tensor(trainer.predict(ds_va).predictions)\n",
    "        va_labels = torch.tensor(y[va_idx])\n",
    "\n",
    "    temperature = torch.nn.Parameter(torch.ones(()))\n",
    "    opt = torch.optim.LBFGS([temperature], lr=0.1, max_iter=50)\n",
    "\n",
    "    def nll_with_temperature():\n",
    "        opt.zero_grad()\n",
    "        scaled = va_logits / temperature.clamp_min(1e-3)\n",
    "        loss = torch.nn.functional.cross_entropy(scaled, va_labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(nll_with_temperature)\n",
    "    T = float(temperature.detach().cpu().numpy())\n",
    "    print(f'Fitted temperature: {T:.3f}')\n",
    "\n",
    "    # OOF-like predictions via simple CV (one split used above); approximate OOF by combining tr/va\n",
    "    # For simplicity we will treat validation as OOF and train part as model predictions on train subset.\n",
    "    with torch.no_grad():\n",
    "        tr_logits = torch.tensor(trainer.predict(ds_tr).predictions)\n",
    "        tr_probs = torch.softmax(tr_logits / T, dim=1).numpy()\n",
    "        va_probs = torch.softmax(va_logits / T, dim=1).numpy()\n",
    "    lora_oof = np.zeros((len(train_df), 3), dtype=float)\n",
    "    lora_oof[tr_idx] = tr_probs\n",
    "    lora_oof[va_idx] = va_probs\n",
    "    print('LoRA pseudo-OOF log_loss:', log_loss(y, lora_oof, labels=classes))\n",
    "\n",
    "    # Test predictions\n",
    "    with torch.no_grad():\n",
    "        test_logits = torch.tensor(trainer.predict(tokenized_test).predictions)\n",
    "        lora_test_proba = torch.softmax(test_logits / T, dim=1).numpy()\n",
    "else:\n",
    "    print('Skipping LoRA training (RUN_LORA=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3c67c",
   "metadata": {},
   "source": [
    "## Fit final models on full data and produce submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Lexical: choose best calibration and train on full data\n",
    "lex_best_models = lex_sigmoid_models if lex_best_name=='sigmoid' else lex_isotonic_models\n",
    "lex_best_scalers = lex_sigmoid_scalers if lex_best_name=='sigmoid' else lex_isotonic_scalers\n",
    "# Refit: use all folds' models+scalers to average predictions on test\n",
    "lex_proba_test_list = []\n",
    "for clf, scaler in zip(lex_best_models, lex_best_scalers):\n",
    "    Xs = scaler.transform(X_lex_test)\n",
    "    lex_proba_test_list.append(clf.predict_proba(Xs))\n",
    "lex_proba_test = np.mean(lex_proba_test_list, axis=0)\n",
    "\n",
    "sub_lex = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': lex_proba_test[:,0],\n",
    "    'winner_model_b': lex_proba_test[:,1],\n",
    "    'winner_tie':     lex_proba_test[:,2],\n",
    "})\n",
    "sub_lex.to_csv('submission_step3_lexical_calibrated.csv', index=False)\n",
    "print('Saved submission_step3_lexical_calibrated.csv')\n",
    "\n",
    "# 2) Embeddings: choose best calibration and train on full data\n",
    "emb_best_models = emb_sigmoid_models if emb_best_name=='sigmoid' else emb_isotonic_models\n",
    "emb_best_scalers = emb_sigmoid_scalers if emb_best_name=='sigmoid' else emb_isotonic_scalers\n",
    "emb_proba_test_list = []\n",
    "for clf, scaler in zip(emb_best_models, emb_best_scalers):\n",
    "    Xs = scaler.transform(X_emb_test)\n",
    "    emb_proba_test_list.append(clf.predict_proba(Xs))\n",
    "emb_proba_test = np.mean(emb_proba_test_list, axis=0)\n",
    "\n",
    "sub_emb = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': emb_proba_test[:,0],\n",
    "    'winner_model_b': emb_proba_test[:,1],\n",
    "    'winner_tie':     emb_proba_test[:,2],\n",
    "})\n",
    "sub_emb.to_csv('submission_step3_embeddings_calibrated.csv', index=False)\n",
    "print('Saved submission_step3_embeddings_calibrated.csv')\n",
    "\n",
    "# 3) LoRA submission if available\n",
    "if lora_test_proba is not None:\n",
    "    sub_lora = pd.DataFrame({\n",
    "        'id': test_df['id'].values,\n",
    "        'winner_model_a': lora_test_proba[:,0],\n",
    "        'winner_model_b': lora_test_proba[:,1],\n",
    "        'winner_tie':     lora_test_proba[:,2],\n",
    "    })\n",
    "    sub_lora.to_csv('submission_step3_lora.csv', index=False)\n",
    "    print('Saved submission_step3_lora.csv')\n",
    "\n",
    "# 4) Ensemble using OOF-optimal weight between lexical and embeddings\n",
    "blend_test = w_blend * lex_proba_test + (1 - w_blend) * emb_proba_test\n",
    "sub_blend = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': blend_test[:,0],\n",
    "    'winner_model_b': blend_test[:,1],\n",
    "    'winner_tie':     blend_test[:,2],\n",
    "})\n",
    "sub_blend.to_csv('submission_step3_ensemble.csv', index=False)\n",
    "print('Saved submission_step3_ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f9d9d",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- To enable LoRA fine-tuning, set `RUN_LORA = True` in the LoRA cell.\n",
    "- LoRA section uses PEFT; ensure `peft`, `transformers`, `datasets`, and `torch` are installed.\n",
    "- Calibrated models use scikit-learn's `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods tested via OOF.\n",
    "- The ensemble weight is found by minimizing OOF log_loss over a simple 1D grid.\n",
    "- All submissions are written to the working directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
