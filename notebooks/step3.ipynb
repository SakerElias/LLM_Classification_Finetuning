{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23592e9",
   "metadata": {},
   "source": [
    "# Step 3 — Extensions: Bias features, Calibration, Ensembling, and LoRA fine-tuning\n",
    "\n",
    "This notebook implements the full Step 3 pipeline without touching prior notebooks:\n",
    "\n",
    "- Bias-aware lexical features (verbosity and structure)\n",
    "- Calibrated classifiers (sigmoid and isotonic)\n",
    "- Embeddings-based model (reusing precomputed .npy when available)\n",
    "- Simple ensembling via OOF-weight search\n",
    "- Optional lightweight LoRA fine-tuning with temperature scaling\n",
    "\n",
    "Outputs: submission CSVs for each component and a blended ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61179c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & setup\n",
    "import re, ast, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA = Path('../data')\n",
    "TRAIN_PATH = DATA / 'train.csv'\n",
    "TEST_PATH = DATA / 'test.csv'\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Build 3-class target: 0=A, 1=B, 2=Tie\n",
    "y = np.select([train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1], [0,1,2])\n",
    "train_df['target'] = y\n",
    "classes = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aee06ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction utilities\n",
    "def extract_text_from_field(text_field):\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text_field)\n",
    "        return ' '.join(parsed) if isinstance(parsed, list) else str(parsed)\n",
    "    except Exception:\n",
    "        return str(text_field)\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n",
    "    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n",
    "    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n",
    "    df['text_a'] = df['prompt_text'] + ' [SEP] ' + df['response_a_text']\n",
    "    df['text_b'] = df['prompt_text'] + ' [SEP] ' + df['response_b_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a26d75",
   "metadata": {},
   "source": [
    "## Bias-aware and structural lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57487ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 11), (3, 11))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Structural counters\n",
    "\n",
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "\n",
    "def paragraph_count(t):\n",
    "    return t.count('\\n\\n') if isinstance(t, str) else 0\n",
    "\n",
    "\n",
    "def list_count(t):\n",
    "    return count_pattern(t, r'(^\\s*[\\-\\*•]\\s|\\d+\\.)')\n",
    "\n",
    "\n",
    "def quote_count(t):\n",
    "    return count_pattern(t, r'>|\\*\\*')\n",
    "\n",
    "\n",
    "def sentence_count(t):\n",
    "    return count_pattern(t, r'[.!?](\\s|$)')\n",
    "\n",
    "\n",
    "def code_block_count(t):\n",
    "    return count_pattern(t, r'```|`[^`]+`')\n",
    "\n",
    "\n",
    "def heading_count(t):\n",
    "    return count_pattern(t, r'^(#|##|###|####|#####|######)\\s')\n",
    "\n",
    "\n",
    "def word_count(t):\n",
    "    return len(t.split()) if isinstance(t, str) else 0\n",
    "\n",
    "# Compute per-side features using explicit suffix mapping ('a' / 'b')\n",
    "for df in (train_df, test_df):\n",
    "    for col, suffix in [('response_a_text', 'a'), ('response_b_text', 'b')]:\n",
    "        df[f'len_{suffix}']   = df[col].astype(str).apply(len)\n",
    "        df[f'wc_{suffix}']    = df[col].astype(str).apply(word_count)\n",
    "        df[f'sent_{suffix}']  = df[col].apply(sentence_count)\n",
    "        df[f'para_{suffix}']  = df[col].apply(paragraph_count)\n",
    "        df[f'list_{suffix}']  = df[col].apply(list_count)\n",
    "        df[f'quote_{suffix}'] = df[col].apply(quote_count)\n",
    "        df[f'code_{suffix}']  = df[col].apply(code_block_count)\n",
    "        df[f'hdr_{suffix}']   = df[col].apply(heading_count)\n",
    "\n",
    "    # Diffs (A - B) — captures verbosity and structure bias\n",
    "    for base in ['len','wc','sent','para','list','quote','code','hdr']:\n",
    "        df[f'{base}_diff'] = df[f'{base}_a'] - df[f'{base}_b']\n",
    "\n",
    "    # Ratios (A / (B+1)) to capture scale-invariant verbosity bias\n",
    "    for base in ['len','wc','sent']:\n",
    "        df[f'{base}_ratio'] = df[f'{base}_a'] / (df[f'{base}_b'] + 1.0)\n",
    "\n",
    "LEX_FEATURES = [\n",
    "    'len_diff','wc_diff','sent_diff','para_diff','list_diff','quote_diff','code_diff','hdr_diff',\n",
    "    'len_ratio','wc_ratio','sent_ratio'\n",
    "]\n",
    "X_lex = train_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex_test = test_df[LEX_FEATURES].fillna(0).astype(float)\n",
    "X_lex.shape, X_lex_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1f8a7",
   "metadata": {},
   "source": [
    "## Calibrated lexical model (sigmoid and isotonic) with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9917e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lex sigmoid] Fold 1: log_loss=1.06936\n",
      "[Lex sigmoid] Fold 2: log_loss=1.06919\n",
      "[Lex sigmoid] Fold 3: log_loss=1.07030\n",
      "[Lex sigmoid] Fold 4: log_loss=1.07064\n",
      "[Lex sigmoid] Fold 5: log_loss=1.06862\n",
      "[Lex sigmoid] OOF log_loss: 1.06962\n",
      "[Lex isotonic] Fold 1: log_loss=1.06078\n",
      "[Lex isotonic] Fold 2: log_loss=1.06198\n",
      "[Lex isotonic] Fold 3: log_loss=1.06130\n",
      "[Lex isotonic] Fold 4: log_loss=1.05976\n",
      "[Lex isotonic] Fold 5: log_loss=1.05932\n",
      "[Lex isotonic] OOF log_loss: 1.06063\n",
      "Best lexical calibration: isotonic OOF log_loss= 1.0606255537663687\n"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs(X, y, base_model=None, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "    if base_model is None:\n",
    "        base_model = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models = []\n",
    "    scalers = []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        clf = CalibratedClassifierCV(estimator=base_model, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Lex {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf)\n",
    "        scalers.append(scaler)\n",
    "    print(f'[Lex {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_lex_sigmoid, lex_sigmoid_models, lex_sigmoid_scalers = cv_calibrated_probs(X_lex, y, method='sigmoid')\n",
    "oof_lex_isotonic, lex_isotonic_models, lex_isotonic_scalers = cv_calibrated_probs(X_lex, y, method='isotonic')\n",
    "\n",
    "# Choose the better calibration by OOF loss\n",
    "lex_oof_list = [('sigmoid', oof_lex_sigmoid), ('isotonic', oof_lex_isotonic)]\n",
    "lex_best_name, lex_best_oof = min(lex_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best lexical calibration:', lex_best_name, 'OOF log_loss=', log_loss(y, lex_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6260c41",
   "metadata": {},
   "source": [
    "## Embedding features (reuse precomputed .npy if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e91be0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57477, 768), (3, 768))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_A_TRAIN = Path('train_embeddings_a.npy')\n",
    "EMBED_B_TRAIN = Path('train_embeddings_b.npy')\n",
    "EMBED_A_TEST  = Path('test_embeddings_a.npy')\n",
    "EMBED_B_TEST  = Path('test_embeddings_b.npy')\n",
    "\n",
    "def ensure_embeddings(train_df, test_df, batch_size=32, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_a = model.encode(train_df['text_a'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    train_b = model.encode(train_df['text_b'].tolist(), batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_a  = model.encode(test_df['text_a'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    test_b  = model.encode(test_df['text_b'].tolist(),  batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    np.save(EMBED_A_TRAIN, train_a); np.save(EMBED_B_TRAIN, train_b)\n",
    "    np.save(EMBED_A_TEST,  test_a);  np.save(EMBED_B_TEST,  test_b)\n",
    "    return train_a, train_b, test_a, test_b\n",
    "\n",
    "if EMBED_A_TRAIN.exists() and EMBED_B_TRAIN.exists() and EMBED_A_TEST.exists() and EMBED_B_TEST.exists():\n",
    "    train_a = np.load(EMBED_A_TRAIN)\n",
    "    train_b = np.load(EMBED_B_TRAIN)\n",
    "    test_a  = np.load(EMBED_A_TEST)\n",
    "    test_b  = np.load(EMBED_B_TEST)\n",
    "else:\n",
    "    train_a, train_b, test_a, test_b = ensure_embeddings(train_df, test_df)\n",
    "\n",
    "X_emb = np.concatenate([train_a, train_b], axis=1)\n",
    "X_emb_test = np.concatenate([test_a, test_b], axis=1)\n",
    "X_emb.shape, X_emb_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d18348",
   "metadata": {},
   "source": [
    "## Calibrated embeddings model with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df93cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emb sigmoid] Fold 1: log_loss=1.06671\n",
      "[Emb sigmoid] Fold 2: log_loss=1.06958\n",
      "[Emb sigmoid] Fold 3: log_loss=1.06626\n",
      "[Emb sigmoid] Fold 4: log_loss=1.06707\n",
      "[Emb sigmoid] Fold 5: log_loss=1.06730\n",
      "[Emb sigmoid] OOF log_loss: 1.06738\n",
      "[Emb isotonic] Fold 1: log_loss=1.06589\n",
      "[Emb isotonic] Fold 2: log_loss=1.06898\n",
      "[Emb isotonic] Fold 3: log_loss=1.06579\n",
      "[Emb isotonic] Fold 4: log_loss=1.06626\n",
      "[Emb isotonic] Fold 5: log_loss=1.06662\n",
      "[Emb isotonic] OOF log_loss: 1.06671\n",
      "Best embedding calibration: isotonic OOF log_loss= 1.0667077305434078\n"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs_numpy(X, y, method='sigmoid', n_splits=5, seed=RANDOM_STATE):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models, scalers = [], []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X[tr], X[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        base = LogisticRegression(max_iter=2000, C=1.0, random_state=seed)\n",
    "        clf = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Emb {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf); scalers.append(scaler)\n",
    "    print(f'[Emb {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_emb_sigmoid, emb_sigmoid_models, emb_sigmoid_scalers = cv_calibrated_probs_numpy(X_emb, y, method='sigmoid')\n",
    "oof_emb_isotonic, emb_isotonic_models, emb_isotonic_scalers = cv_calibrated_probs_numpy(X_emb, y, method='isotonic')\n",
    "\n",
    "emb_oof_list = [('sigmoid', oof_emb_sigmoid), ('isotonic', oof_emb_isotonic)]\n",
    "emb_best_name, emb_best_oof = min(emb_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best embedding calibration:', emb_best_name, 'OOF log_loss=', log_loss(y, emb_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c54b32",
   "metadata": {},
   "source": [
    "## Simple ensemble (OOF-weight search on lexical + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf63b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble best weight (lexical): 0.59, OOF log_loss: 1.05411\n"
     ]
    }
   ],
   "source": [
    "# Grid search weights w in [0..1] for p = w*p_lex + (1-w)*p_emb minimizing OOF log_loss\n",
    "def best_weight_for_blend(y, p_lex, p_emb, steps=101):\n",
    "    best_w, best_loss = 0.5, 1e9\n",
    "    for i in range(steps):\n",
    "        w = i/(steps-1)\n",
    "        blend = w*p_lex + (1-w)*p_emb\n",
    "        loss = log_loss(y, blend, labels=classes)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss; best_w = w\n",
    "    return best_w, best_loss\n",
    "\n",
    "w_blend, loss_blend = best_weight_for_blend(y, lex_best_oof, emb_best_oof)\n",
    "print(f'Ensemble best weight (lexical): {w_blend:.2f}, OOF log_loss: {loss_blend:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e491d",
   "metadata": {},
   "source": [
    "## Optional: Lightweight LoRA fine-tuning with temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a10c07fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping LoRA training (RUN_LORA=False).\n"
     ]
    }
   ],
   "source": [
    "RUN_LORA = False  # set True to train (can be slow)\n",
    "LORA_MODEL_NAME = 'distilbert-base-uncased'  # small and fast; change to 'microsoft/deberta-v3-small' if desired\n",
    "\n",
    "lora_oof = None\n",
    "lora_test_proba = None\n",
    "\n",
    "if RUN_LORA:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    from datasets import Dataset\n",
    "\n",
    "    # Build text inputs for classification (prompt + A + B)\n",
    "    def build_input(df):\n",
    "        return (\n",
    "            '[PROMPT] ' + df['prompt_text'] +\n",
    "            ' [A] ' + df['response_a_text'] +\n",
    "            ' [B] ' + df['response_b_text']\n",
    "        )\n",
    "\n",
    "    train_inputs = build_input(train_df)\n",
    "    test_inputs  = build_input(test_df)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL_NAME)\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "    ds_train = Dataset.from_pandas(pd.DataFrame({'text': train_inputs, 'label': y}))\n",
    "    ds_test  = Dataset.from_pandas(pd.DataFrame({'text': test_inputs}))\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL_NAME, num_labels=3)\n",
    "\n",
    "    # Auto-detect common attention module names for LoRA targets\n",
    "    target_keywords = ['q_proj','v_proj','k_proj','query','key','value','q_lin','v_lin']\n",
    "    all_module_names = [n for n,_ in model.named_modules()]\n",
    "    target_modules = sorted({n.split('.')[-1] for n in all_module_names if any(k in n for k in target_keywords)})\n",
    "    if not target_modules:\n",
    "        # fallback for DistilBERT attention names\n",
    "        target_modules = ['q_lin','v_lin']\n",
    "\n",
    "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, task_type='SEQ_CLS', target_modules=target_modules)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    tokenized_train = ds_train.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "    tokenized_test  = ds_test.map(tokenize_fn, batched=True, remove_columns=['text'])\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        return {'log_loss': log_loss(labels, probs, labels=classes)}\n",
    "\n",
    "    # Simple split for evaluation and temperature fitting\n",
    "    tr_idx, va_idx = train_test_split(np.arange(len(ds_train)), test_size=0.15, random_state=RANDOM_STATE, stratify=y)\n",
    "    ds_tr = tokenized_train.select(tr_idx.tolist())\n",
    "    ds_va = tokenized_train.select(va_idx.tolist())\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir='out_lora',\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=200,\n",
    "        logging_steps=100,\n",
    "        save_strategy='no',\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=ds_tr, eval_dataset=ds_va, compute_metrics=compute_metrics)\n",
    "    trainer.train()\n",
    "\n",
    "    # Temperature scaling on validation logits\n",
    "    with torch.no_grad():\n",
    "        va_logits = torch.tensor(trainer.predict(ds_va).predictions)\n",
    "        va_labels = torch.tensor(y[va_idx])\n",
    "\n",
    "    temperature = torch.nn.Parameter(torch.ones(()))\n",
    "    opt = torch.optim.LBFGS([temperature], lr=0.1, max_iter=50)\n",
    "\n",
    "    def nll_with_temperature():\n",
    "        opt.zero_grad()\n",
    "        scaled = va_logits / temperature.clamp_min(1e-3)\n",
    "        loss = torch.nn.functional.cross_entropy(scaled, va_labels)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    opt.step(nll_with_temperature)\n",
    "    T = float(temperature.detach().cpu().numpy())\n",
    "    print(f'Fitted temperature: {T:.3f}')\n",
    "\n",
    "    # OOF-like predictions via simple CV (one split used above); approximate OOF by combining tr/va\n",
    "    # For simplicity we will treat validation as OOF and train part as model predictions on train subset.\n",
    "    with torch.no_grad():\n",
    "        tr_logits = torch.tensor(trainer.predict(ds_tr).predictions)\n",
    "        tr_probs = torch.softmax(tr_logits / T, dim=1).numpy()\n",
    "        va_probs = torch.softmax(va_logits / T, dim=1).numpy()\n",
    "    lora_oof = np.zeros((len(train_df), 3), dtype=float)\n",
    "    lora_oof[tr_idx] = tr_probs\n",
    "    lora_oof[va_idx] = va_probs\n",
    "    print('LoRA pseudo-OOF log_loss:', log_loss(y, lora_oof, labels=classes))\n",
    "\n",
    "    # Test predictions\n",
    "    with torch.no_grad():\n",
    "        test_logits = torch.tensor(trainer.predict(tokenized_test).predictions)\n",
    "        lora_test_proba = torch.softmax(test_logits / T, dim=1).numpy()\n",
    "else:\n",
    "    print('Skipping LoRA training (RUN_LORA=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3c67c",
   "metadata": {},
   "source": [
    "## Fit final models on full data and produce submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1485d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_step3_lexical_calibrated.csv\n",
      "Saved submission_step3_embeddings_calibrated.csv\n",
      "Saved submission_step3_ensemble.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Lexical: choose best calibration and train on full data\n",
    "lex_best_models = lex_sigmoid_models if lex_best_name=='sigmoid' else lex_isotonic_models\n",
    "lex_best_scalers = lex_sigmoid_scalers if lex_best_name=='sigmoid' else lex_isotonic_scalers\n",
    "# Refit: use all folds' models+scalers to average predictions on test\n",
    "lex_proba_test_list = []\n",
    "for clf, scaler in zip(lex_best_models, lex_best_scalers):\n",
    "    Xs = scaler.transform(X_lex_test)\n",
    "    lex_proba_test_list.append(clf.predict_proba(Xs))\n",
    "lex_proba_test = np.mean(lex_proba_test_list, axis=0)\n",
    "\n",
    "sub_lex = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': lex_proba_test[:,0],\n",
    "    'winner_model_b': lex_proba_test[:,1],\n",
    "    'winner_tie':     lex_proba_test[:,2],\n",
    "})\n",
    "sub_lex.to_csv('submission_step3_lexical_calibrated.csv', index=False)\n",
    "print('Saved submission_step3_lexical_calibrated.csv')\n",
    "\n",
    "# 2) Embeddings: choose best calibration and train on full data\n",
    "emb_best_models = emb_sigmoid_models if emb_best_name=='sigmoid' else emb_isotonic_models\n",
    "emb_best_scalers = emb_sigmoid_scalers if emb_best_name=='sigmoid' else emb_isotonic_scalers\n",
    "emb_proba_test_list = []\n",
    "for clf, scaler in zip(emb_best_models, emb_best_scalers):\n",
    "    Xs = scaler.transform(X_emb_test)\n",
    "    emb_proba_test_list.append(clf.predict_proba(Xs))\n",
    "emb_proba_test = np.mean(emb_proba_test_list, axis=0)\n",
    "\n",
    "sub_emb = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': emb_proba_test[:,0],\n",
    "    'winner_model_b': emb_proba_test[:,1],\n",
    "    'winner_tie':     emb_proba_test[:,2],\n",
    "})\n",
    "sub_emb.to_csv('submission_step3_embeddings_calibrated.csv', index=False)\n",
    "print('Saved submission_step3_embeddings_calibrated.csv')\n",
    "\n",
    "# 3) LoRA submission if available\n",
    "if lora_test_proba is not None:\n",
    "    sub_lora = pd.DataFrame({\n",
    "        'id': test_df['id'].values,\n",
    "        'winner_model_a': lora_test_proba[:,0],\n",
    "        'winner_model_b': lora_test_proba[:,1],\n",
    "        'winner_tie':     lora_test_proba[:,2],\n",
    "    })\n",
    "    sub_lora.to_csv('submission_step3_lora.csv', index=False)\n",
    "    print('Saved submission_step3_lora.csv')\n",
    "\n",
    "# 4) Ensemble using OOF-optimal weight between lexical and embeddings\n",
    "blend_test = w_blend * lex_proba_test + (1 - w_blend) * emb_proba_test\n",
    "sub_blend = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': blend_test[:,0],\n",
    "    'winner_model_b': blend_test[:,1],\n",
    "    'winner_tie':     blend_test[:,2],\n",
    "})\n",
    "sub_blend.to_csv('submission_step3_ensemble.csv', index=False)\n",
    "print('Saved submission_step3_ensemble.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f9d9d",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- To enable LoRA fine-tuning, set `RUN_LORA = True` in the LoRA cell.\n",
    "- LoRA section uses PEFT; ensure `peft`, `transformers`, `datasets`, and `torch` are installed.\n",
    "- Calibrated models use scikit-learn's `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods tested via OOF.\n",
    "- The ensemble weight is found by minimizing OOF log_loss over a simple 1D grid.\n",
    "- All submissions are written to the working directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
