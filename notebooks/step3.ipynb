{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23592e9",
   "metadata": {},
   "source": [
    "# Step 3 — Extensions: Bias features, Calibration, Ensembling, and LoRA fine-tuning\n",
    "\n",
    "This notebook implements the full Step 3 pipeline\n",
    "\n",
    "- Bias-aware lexical features (verbosity and structure)\n",
    "- Calibrated classifiers (sigmoid and isotonic)\n",
    "- Embeddings-based model (reusing precomputed .npy when available)\n",
    "- Simple ensembling via OOF-weight search\n",
    "- Optional lightweight LoRA fine-tuning with temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "61179c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os, re, warnings, time, ast, platform, multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb8c6a",
   "metadata": {},
   "source": [
    "# 0) CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c27866dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose exactly one:\n",
    "#   \"lexical\"          → Step 1 features + LR (calibrated)\n",
    "#   \"embeddings\"       → SentenceTransformer (e.g., MiniLM) + classifier (calibrated)\n",
    "#   \"embeddings_lora\"  → DeBERTa + LoRA adapters (single-model classifier)\n",
    "#   \"ensemble\"         → blend lexical + one embeddings branch\n",
    "MODE = \"embeddings\"   # \"lexical\" | \"embeddings\" | \"embeddings_lora\" | \"ensemble\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "SEED     = 42\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "OUT_DIR  = Path(\"../outputs\");  OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ART_DIR  = Path(\"../artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
    "TEST_PATH  = DATA_DIR / \"test.csv\"\n",
    "\n",
    "# Used only if MODE == \"embeddings\" (SentenceTransformer backbone)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# Used only if MODE == \"embeddings_lora\" (SentenceTransformer backbone)\n",
    "LORA_MODEL = \"microsoft/deberta-v3-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0f156",
   "metadata": {},
   "source": [
    "# 1) DATA LOADING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aee06ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "# Build 3-class target: 0=A wins, 1=B wins, 2=Tie\n",
    "y = np.select(\n",
    " [train_df['winner_model_a']==1, train_df['winner_model_b']==1,\n",
    "train_df['winner_tie']==1],\n",
    " [0, 1, 2]\n",
    ")\n",
    "train_df['target'] = y\n",
    "\n",
    "classes = [0, 1, 2]\n",
    "# Normalize JSON-ish fields and build text_a / text_b\n",
    "def extract_text_from_field(v): \n",
    "    try: \n",
    "        parsed = ast.literal_eval(v) \n",
    "        return \" \".join(map(str, parsed)) if isinstance(parsed, list) else str(parsed) \n",
    "    except Exception:\n",
    "        return str(v)\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"prompt_text\"]     = df[\"prompt\"].apply(extract_text_from_field)\n",
    "    df[\"response_a_text\"] = df[\"response_a\"].apply(extract_text_from_field)\n",
    "    df[\"response_b_text\"] = df[\"response_b\"].apply(extract_text_from_field)\n",
    "    df[\"text_a\"] = df[\"prompt_text\"] + \" [SEP] \" + df[\"response_a_text\"]\n",
    "    df[\"text_b\"] = df[\"prompt_text\"] + \" [SEP] \" + df[\"response_b_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df794a68",
   "metadata": {},
   "source": [
    "# 2) HELPERS (CV, calibration, prediction, blending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24ee1a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_indices = [(tr, va) for tr, va in skf.split(np.zeros(len(y)), y)]\n",
    "\n",
    "def cv_calibrated_probs_numpy(X, y, method=\"sigmoid\"):\n",
    "    \"\"\"Return OOF probs + list of fold models & scalers.\"\"\"\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models, scalers = [], []\n",
    "    for fold, (tr, va) in enumerate(fold_indices, 1):\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(X[tr])\n",
    "        Xva_s = scaler.transform(X[va])\n",
    "\n",
    "        base = LogisticRegression(\n",
    "            max_iter=2000, C=1.0, random_state=SEED,\n",
    "            solver=\"lbfgs\", multi_class=\"multinomial\", n_jobs=1\n",
    "            )\n",
    "        \n",
    "        clf  = CalibratedClassifierCV(estimator=base, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, y[tr])\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        print(f\"[CV {method}] fold {fold} logloss = {log_loss(y[va], proba, labels=classes):.5f}\")\n",
    "        models.append(clf); scalers.append(scaler)\n",
    "    print(f\"[CV {method}] OOF logloss = {log_loss(y, oof, labels=classes):.5f}\")\n",
    "    return oof, models, scalers\n",
    "\n",
    "def predict_from_folds(models, scalers, X):\n",
    "    P = np.zeros((len(X), 3), dtype=float)\n",
    "    for clf, scaler in zip(models, scalers):\n",
    "        Xt = scaler.transform(X)\n",
    "        P += clf.predict_proba(Xt) / len(models)\n",
    "    return P\n",
    "\n",
    "def best_weight_for_blend(y, p1, p2, steps=101):\n",
    "    best_w, best_loss = 0.5, 1e9\n",
    "    for i in range(steps):\n",
    "        w = i/(steps-1)\n",
    "        blend = w*p1 + (1-w)*p2\n",
    "        loss  = log_loss(y, blend, labels=classes)\n",
    "        if loss < best_loss:\n",
    "            best_loss, best_w = loss, w\n",
    "    return best_w, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a26d75",
   "metadata": {},
   "source": [
    "## Bias-aware and structural lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57487ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str): return 0\n",
    "    return len(re.findall(pattern, text, flags=re.MULTILINE))\n",
    "\n",
    "def paragraph_count(t): return t.count(\"\\n\\n\") if isinstance(t, str) else 0\n",
    "def list_count(t):      return count_pattern(t, r\"^\\s*(?:[\\-\\*•]\\s|\\d+\\.)\")\n",
    "def quote_count(t):     return count_pattern(t, r\"(^>\\s|(?<!\\*)\\*\\*[^*]+\\*\\*)\")\n",
    "def sentence_count(t):  return count_pattern(t, r\"[.!?](?:\\s|$)\")\n",
    "def code_block_count(t):return count_pattern(t, r\"```|`[^`]+`\")\n",
    "def heading_count(t):   return count_pattern(t, r\"^(?:#{1,6})\\s\")\n",
    "def word_count(t):      return len(t.split()) if isinstance(t, str) else 0\n",
    "\n",
    "def build_lex_features(df, a_col=\"response_a_text\", b_col=\"response_b_text\"):\n",
    "    A = df[a_col].fillna(\"\").astype(str); B = df[b_col].fillna(\"\").astype(str)\n",
    "    feats = {}\n",
    "    for tag, series in [(\"a\", A), (\"b\", B)]:\n",
    "        feats[f\"len_{tag}\"]   = series.map(len)\n",
    "        feats[f\"wc_{tag}\"]    = series.map(word_count)\n",
    "        feats[f\"sent_{tag}\"]  = series.map(sentence_count)\n",
    "        feats[f\"para_{tag}\"]  = series.map(paragraph_count)\n",
    "        feats[f\"list_{tag}\"]  = series.map(list_count)\n",
    "        feats[f\"quote_{tag}\"] = series.map(quote_count)\n",
    "        feats[f\"code_{tag}\"]  = series.map(code_block_count)\n",
    "        feats[f\"hdr_{tag}\"]   = series.map(heading_count)\n",
    "    F = pd.DataFrame(feats).astype(float)\n",
    "\n",
    "    # diffs\n",
    "    for base in [\"len\",\"wc\",\"sent\",\"para\",\"list\",\"quote\",\"code\",\"hdr\"]:\n",
    "        F[f\"{base}_diff\"] = F[f\"{base}_a\"] - F[f\"{base}_b\"]\n",
    "    # ratios\n",
    "    for base in [\"len\",\"wc\",\"sent\"]:\n",
    "        F[f\"{base}_ratio\"] = (F[f\"{base}_a\"] + 1.0) / (F[f\"{base}_b\"] + 1.0)\n",
    "\n",
    "    eps = 1e-6\n",
    "    for base in [\"sent\",\"para\",\"list\",\"quote\",\"code\",\"hdr\"]:\n",
    "        F[f\"{base}_per100w_a\"] = 100.0 * F[f\"{base}_a\"] / (F[\"wc_a\"] + eps)\n",
    "        F[f\"{base}_per100w_b\"] = 100.0 * F[f\"{base}_b\"] / (F[\"wc_b\"] + eps)\n",
    "        F[f\"{base}_per100w_diff\"] = F[f\"{base}_per100w_a\"] - F[f\"{base}_per100w_b\"]\n",
    "    return F.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "lex_cols_keep = [\n",
    "    \"len_diff\",\"wc_diff\",\"sent_diff\",\"para_diff\",\"list_diff\",\"quote_diff\",\"code_diff\",\"hdr_diff\",\n",
    "    \"len_ratio\",\"wc_ratio\",\"sent_ratio\"\n",
    "]\n",
    "\n",
    "X_lex_train_df = build_lex_features(train_df)\n",
    "X_lex_test_df  = build_lex_features(test_df)\n",
    "X_lex      = X_lex_train_df[lex_cols_keep].copy().astype(float)\n",
    "X_lex_test = X_lex_test_df[lex_cols_keep].copy().astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1f8a7",
   "metadata": {},
   "source": [
    "## Calibrated lexical model (sigmoid and isotonic) with OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9917e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lex sigmoid] Fold 1: log_loss=1.07004\n",
      "[Lex sigmoid] Fold 2: log_loss=1.06940\n",
      "[Lex sigmoid] Fold 3: log_loss=1.07066\n",
      "[Lex sigmoid] Fold 4: log_loss=1.07088\n",
      "[Lex sigmoid] Fold 5: log_loss=1.06935\n",
      "[Lex sigmoid] OOF log_loss: 1.07007\n",
      "[Lex isotonic] Fold 1: log_loss=1.06096\n",
      "[Lex isotonic] Fold 2: log_loss=1.06248\n",
      "[Lex isotonic] Fold 3: log_loss=1.06179\n",
      "[Lex isotonic] Fold 4: log_loss=1.06018\n",
      "[Lex isotonic] Fold 5: log_loss=1.06076\n",
      "[Lex isotonic] OOF log_loss: 1.06124\n",
      "Best lexical calibration: isotonic OOF log_loss= 1.0612351518282728\n"
     ]
    }
   ],
   "source": [
    "def cv_calibrated_probs(X, y, base_model=None, method='sigmoid', n_splits=5, seed=SEED):\n",
    "    if base_model is None:\n",
    "        base_model = LogisticRegression(\n",
    "    max_iter=2000, C=1.0, random_state=SEED,\n",
    "    solver=\"lbfgs\", multi_class=\"multinomial\", n_jobs=1\n",
    "    )\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((len(X), 3), dtype=float)\n",
    "    models = []\n",
    "    scalers = []\n",
    "    for fold, (tr, va) in enumerate(skf.split(X, y), 1):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y[tr], y[va]\n",
    "        scaler = StandardScaler()\n",
    "        Xtr_s = scaler.fit_transform(Xtr)\n",
    "        Xva_s = scaler.transform(Xva)\n",
    "        clf = CalibratedClassifierCV(estimator=base_model, method=method, cv=3)\n",
    "        clf.fit(Xtr_s, ytr)\n",
    "        proba = clf.predict_proba(Xva_s)\n",
    "        oof[va] = proba\n",
    "        loss = log_loss(yva, proba, labels=classes)\n",
    "        print(f'[Lex {method}] Fold {fold}: log_loss={loss:.5f}')\n",
    "        models.append(clf)\n",
    "        scalers.append(scaler)\n",
    "    print(f'[Lex {method}] OOF log_loss: {log_loss(y, oof, labels=classes):.5f}')\n",
    "    return oof, models, scalers\n",
    "\n",
    "oof_lex_sigmoid, lex_sigmoid_models, lex_sigmoid_scalers = cv_calibrated_probs(X_lex, y, method='sigmoid')\n",
    "oof_lex_isotonic, lex_isotonic_models, lex_isotonic_scalers = cv_calibrated_probs(X_lex, y, method='isotonic')\n",
    "\n",
    "# Choose the better calibration by OOF loss\n",
    "lex_oof_list = [('sigmoid', oof_lex_sigmoid), ('isotonic', oof_lex_isotonic)]\n",
    "lex_best_name, lex_best_oof = min(lex_oof_list, key=lambda t: log_loss(y, t[1], labels=classes))\n",
    "print('Best lexical calibration:', lex_best_name, 'OOF log_loss=', log_loss(y, lex_best_oof, labels=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a5e09",
   "metadata": {},
   "source": [
    "# 4) LEXICAL MODEL (if MODE in {\"lexical\", \"ensemble\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2cbe5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_best_name = None\n",
    "lex_best_oof = None\n",
    "lex_best_models = []\n",
    "lex_best_scalers = []\n",
    "\n",
    "if MODE in {\"lexical\",\"ensemble\"}:\n",
    "    oof_lex_sig, lex_sig_models, lex_sig_scalers = cv_calibrated_probs_numpy(X_lex.values, y, method=\"sigmoid\")\n",
    "    oof_lex_iso, lex_iso_models, lex_iso_scalers = cv_calibrated_probs_numpy(X_lex.values, y, method=\"isotonic\")\n",
    "\n",
    "    if log_loss(y, oof_lex_iso, labels=classes) < log_loss(y, oof_lex_sig, labels=classes):\n",
    "        lex_best_name, lex_best_oof = \"isotonic\", oof_lex_iso\n",
    "        lex_best_models, lex_best_scalers = lex_iso_models, lex_iso_scalers\n",
    "    else:\n",
    "        lex_best_name, lex_best_oof = \"sigmoid\", oof_lex_sig\n",
    "        lex_best_models, lex_best_scalers = lex_sig_models, lex_sig_scalers\n",
    "\n",
    "    # test preds\n",
    "    lex_proba_test = predict_from_folds(lex_best_models, lex_best_scalers, X_lex_test.values)\n",
    "else:\n",
    "    lex_proba_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6260c41",
   "metadata": {},
   "source": [
    "# 5) EMBEDDINGS (if MODE in {\"embeddings\",\"ensemble\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e91be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2 on cpu\n",
      "[cache] Loaded train embeddings: (57477, 384) (57477, 384)\n",
      "[cache] Loaded test embeddings: (3, 384) (3, 384)\n",
      "Emb shapes: (57477, 768) (3, 768)\n",
      "[CV sigmoid] fold 1 logloss = 1.06671\n",
      "[CV sigmoid] fold 2 logloss = 1.06958\n",
      "[CV sigmoid] fold 3 logloss = 1.06626\n",
      "[CV sigmoid] fold 4 logloss = 1.06707\n",
      "[CV sigmoid] fold 5 logloss = 1.06730\n",
      "[CV sigmoid] OOF logloss = 1.06738\n",
      "[CV isotonic] fold 1 logloss = 1.06589\n",
      "[CV isotonic] fold 2 logloss = 1.06898\n",
      "[CV isotonic] fold 3 logloss = 1.06579\n",
      "[CV isotonic] fold 4 logloss = 1.06626\n",
      "[CV isotonic] fold 5 logloss = 1.06662\n",
      "[CV isotonic] OOF logloss = 1.06671\n"
     ]
    }
   ],
   "source": [
    "X_emb_train = None; X_emb_test = None\n",
    "emb_best_name = None; emb_best_oof = None\n",
    "emb_best_models = []; emb_best_scalers = []\n",
    "\n",
    "if MODE in {\"embeddings\",\"ensemble\"}:\n",
    "    import torch\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    # --- Paths for cached embeddings\n",
    "    ART_DIR = Path(\"../artifacts\"); ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    EMBED_A_TRAIN = ART_DIR / 'train_embeddings_a.npy'\n",
    "    EMBED_B_TRAIN = ART_DIR / 'train_embeddings_b.npy'\n",
    "    EMBED_A_TEST  = ART_DIR / 'test_embeddings_a.npy'\n",
    "    EMBED_B_TEST  = ART_DIR / 'test_embeddings_b.npy'\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    st_model = SentenceTransformer(str(EMBEDDING_MODEL), device=device)\n",
    "\n",
    "    # FORCE the same tokenizer behavior as Kaggle (critical)\n",
    "    slow_tok = AutoTokenizer.from_pretrained(str(EMBEDDING_MODEL), use_fast=False)\n",
    "    st_model._first_module().tokenizer = slow_tok\n",
    "    st_model.max_seq_length = 512\n",
    "    BATCH = 32\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Loading SentenceTransformer:\", EMBEDDING_MODEL, \"on\", device)\n",
    "    st_model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
    "    st_model.max_seq_length = 512\n",
    "    BATCH = 32\n",
    "\n",
    "    def load_or_encode(prefix, a_path, b_path, df):\n",
    "        # Try cache\n",
    "        if a_path.exists() and b_path.exists():\n",
    "            Ea = np.load(a_path)\n",
    "            Eb = np.load(b_path)\n",
    "            if Ea.shape[0] == len(df) and Eb.shape[0] == len(df):\n",
    "                print(f\"[cache] Loaded {prefix} embeddings:\", Ea.shape, Eb.shape)\n",
    "                return Ea, Eb\n",
    "            else:\n",
    "                print(f\"[cache] Shape mismatch for {prefix} cache → recomputing.\")\n",
    "        # Compute and save\n",
    "        start = time.time()\n",
    "        print(f\"[encode] Computing {prefix} embeddings...\")\n",
    "        Ea = st_model.encode(df[\"text_a\"].tolist(), batch_size=BATCH, show_progress_bar=True, convert_to_numpy=True)\n",
    "        Eb = st_model.encode(df[\"text_b\"].tolist(), batch_size=BATCH, show_progress_bar=True, convert_to_numpy=True)\n",
    "        np.save(a_path, Ea); np.save(b_path, Eb)\n",
    "        print(f\"[encode] Saved {prefix} embeddings to {ART_DIR}  (elapsed {(time.time()-start)/60:.2f} min)\")\n",
    "        return Ea, Eb\n",
    "\n",
    "    # --- Train/Test embeddings (A and B)\n",
    "    train_a, train_b = load_or_encode(\"train\", EMBED_A_TRAIN, EMBED_B_TRAIN, train_df)\n",
    "    test_a,  test_b  = load_or_encode(\"test\",  EMBED_A_TEST,  EMBED_B_TEST,  test_df)\n",
    "\n",
    "    # --- concat A‖B\n",
    "    X_emb_train = np.concatenate([train_a, train_b], axis=1)\n",
    "    X_emb_test  = np.concatenate([test_a,  test_b],  axis=1)\n",
    "    print(\"Emb shapes:\", X_emb_train.shape, X_emb_test.shape)\n",
    "\n",
    "    # --- Calibrated CV (best of sigmoid / isotonic)\n",
    "    oof_emb_sig, emb_sig_models, emb_sig_scalers = cv_calibrated_probs_numpy(X_emb_train, y, method=\"sigmoid\")\n",
    "    oof_emb_iso, emb_iso_models, emb_iso_scalers = cv_calibrated_probs_numpy(X_emb_train, y, method=\"isotonic\")\n",
    "\n",
    "    if log_loss(y, oof_emb_iso, labels=classes) < log_loss(y, oof_emb_sig, labels=classes):\n",
    "        emb_best_name, emb_best_oof = \"isotonic\", oof_emb_iso\n",
    "        emb_best_models, emb_best_scalers = emb_iso_models, emb_iso_scalers\n",
    "    else:\n",
    "        emb_best_name, emb_best_oof = \"sigmoid\", oof_emb_sig\n",
    "        emb_best_models, emb_best_scalers = emb_sig_models, emb_sig_scalers\n",
    "\n",
    "    # --- Test-time probs (fold average)\n",
    "    emb_proba_test = predict_from_folds(emb_best_models, emb_best_scalers, X_emb_test)\n",
    "\n",
    "else:\n",
    "    emb_proba_test = None\n",
    "    print(\"MODE does not include embeddings → skipping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e491d",
   "metadata": {},
   "source": [
    "#Lightweight LoRA fine-tuning with temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a10c07fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- text sanitization (yours) ---\n",
    "_SURROGATE_RE = re.compile(r'[\\ud800-\\udfff]')\n",
    "def sanitize_text(x):\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if not isinstance(x, str):\n",
    "        x = str(x)\n",
    "    x = x.replace(\"\\x00\", \"\")\n",
    "    x = _SURROGATE_RE.sub(\"�\", x)\n",
    "    try:\n",
    "        x.encode(\"utf-8\")\n",
    "        return x\n",
    "    except UnicodeEncodeError:\n",
    "        return x.encode(\"utf-8\",\"replace\").decode(\"utf-8\")\n",
    "\n",
    "if MODE in {\"embeddings_lora\", \"ensemble\"}:\n",
    "    from datasets import Dataset\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModelForSequenceClassification,\n",
    "        DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    # 1) Build triplets from normalized Step 1 columns\n",
    "    def _triplet(row):\n",
    "        return f\"{row['prompt_text']} [SEP] {row['response_a_text']} [SEP] {row['response_b_text']}\"\n",
    "\n",
    "    train_texts = [sanitize_text(t) for t in train_df.apply(_triplet, axis=1).tolist()]\n",
    "    test_texts  = [sanitize_text(t) for t in test_df.apply(_triplet,  axis=1).tolist()]\n",
    "\n",
    "    # 2) Split\n",
    "    df_all = pd.DataFrame({\"text\": train_texts, \"label\": y})\n",
    "    tr_df, va_df = train_test_split(df_all, test_size=0.1, random_state=SEED, stratify=df_all[\"label\"])\n",
    "    tr_texts, tr_labels = tr_df[\"text\"].tolist(), tr_df[\"label\"].tolist()\n",
    "    va_texts, va_labels = va_df[\"text\"].tolist(), va_df[\"label\"].tolist()\n",
    "\n",
    "    # 3) HF datasets via from_dict (avoids pandas->arrow unicode pitfalls)\n",
    "    ds_tr = Dataset.from_dict({\"text\": tr_texts, \"labels\": tr_labels})\n",
    "    ds_va = Dataset.from_dict({\"text\": va_texts, \"labels\": va_labels})\n",
    "    ds_te = Dataset.from_dict({\"text\": test_texts})\n",
    "\n",
    "    # 4) Tokenizer (prefer fast; fall back if sentencepiece/tokenizers missing)\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL, use_fast=True)\n",
    "    except Exception:\n",
    "        print(\"Fast tokenizer unavailable → falling back to slow tokenizer.\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LORA_MODEL, use_fast=False)\n",
    "\n",
    "    # Speed: shorter seq length is usually enough for this task\n",
    "    MAX_LEN = 256\n",
    "    def _tok(batch):\n",
    "        return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "    ds_tr = ds_tr.map(_tok, batched=True, remove_columns=[\"text\"])\n",
    "    ds_va = ds_va.map(_tok, batched=True, remove_columns=[\"text\"])\n",
    "    ds_te = ds_te.map(_tok, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    # 5) Base model + LoRA adapters\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(LORA_MODEL, num_labels=3)\n",
    "\n",
    "    # Pick correct target modules for common backbones\n",
    "    lm = LORA_MODEL.lower()\n",
    "    if \"deberta\" in lm:  # e.g. microsoft/deberta-v3-base\n",
    "        target_modules = [\"query_proj\", \"key_proj\", \"value_proj\", \"dense\"]\n",
    "    elif \"roberta\" in lm or \"bert\" in lm:\n",
    "        target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n",
    "    else:\n",
    "        # generic fallback (won't hurt; PEFT skips non-matching names)\n",
    "        target_modules = [\"query\", \"key\", \"value\", \"dense\", \"q_proj\", \"k_proj\", \"v_proj\"]\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "        target_modules=target_modules,\n",
    "        bias=\"none\", task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    # 6) Training\n",
    "    use_fp16 = torch.cuda.is_available()\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(ART_DIR / \"lora_cls\"),\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.06,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=use_fp16, report_to=\"none\", seed=SEED,\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "        return {\"log_loss\": float(_ll(labels, probs, labels=[0,1,2]))}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=args,\n",
    "        train_dataset=ds_tr, eval_dataset=ds_va,\n",
    "        tokenizer=tokenizer, data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # 7) Test predictions\n",
    "    te_logits = trainer.predict(ds_te).predictions\n",
    "    lora_proba_test = torch.softmax(torch.tensor(te_logits), dim=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3c67c",
   "metadata": {},
   "source": [
    "# 7) FINAL SUBMISSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8b1485d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: ..\\outputs\\submission_step3_embeddings_calibrated_isotonic.csv\n"
     ]
    }
   ],
   "source": [
    "def save_submission(name, probs):\n",
    "    sub = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"].values,\n",
    "        \"winner_model_a\": probs[:, 0],\n",
    "        \"winner_model_b\": probs[:, 1],\n",
    "        \"winner_tie\":     probs[:, 2],\n",
    "    })\n",
    "    out = OUT_DIR / f\"submission_step3_{name}.csv\"\n",
    "    sub.to_csv(out, index=False)\n",
    "    print(\"✅ Saved:\", out)\n",
    "\n",
    "if MODE == \"lexical\":\n",
    "    save_submission(f\"lexical_calibrated_{lex_best_name}\", lex_proba_test)\n",
    "\n",
    "elif MODE == \"embeddings\":\n",
    "    save_submission(f\"embeddings_calibrated_{emb_best_name}\", emb_proba_test)\n",
    "\n",
    "elif MODE == \"embeddings_lora\":\n",
    "    save_submission(\"embeddings_lora\", lora_proba_test)\n",
    "\n",
    "elif MODE == \"ensemble\":\n",
    "    # Case A: lexical + MiniLM embeddings (OOF available for both)\n",
    "    if (lex_best_oof is not None) and (emb_best_oof is not None) and (emb_proba_test is not None):\n",
    "        w_blend, loss_blend = best_weight_for_blend(y, lex_best_oof, emb_best_oof)\n",
    "        print(f\"\\n✅ Ensemble (lex + MiniLM) → best w(lex)={w_blend:.2f}, OOF logloss={loss_blend:.5f}\")\n",
    "        blend_test = w_blend * lex_proba_test + (1 - w_blend) * emb_proba_test\n",
    "        save_submission(\"blend_lex_minilm\", blend_test)\n",
    "\n",
    "    # Case B: lexical + LoRA (no OOF for LoRA by default → fixed blend)\n",
    "    elif (lex_proba_test is not None) and (lora_proba_test is not None):\n",
    "        w_blend = 0.5   # or add a tiny train/val split earlier to tune this weight\n",
    "        print(f\"\\n✅ Ensemble (lex + LoRA) → using fixed w(lex)={w_blend:.2f}\")\n",
    "        blend_test = w_blend * lex_proba_test + (1 - w_blend) * lora_proba_test\n",
    "        save_submission(\"blend_lex_lora\", blend_test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Ensemble requires lexical + one embeddings branch (MiniLM or LoRA).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a938fe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save OOF predictions to numpy files\n",
    "np.save('../artifacts/lex_best_oof.npy', lex_best_oof)\n",
    "np.save('../artifacts/emb_best_oof.npy', emb_best_oof)\n",
    "np.save('../artifacts/y_true.npy', y)\n",
    "\n",
    "# Save X_lex for structural feature analysis\n",
    "X_lex.to_csv('../artifacts/X_lex_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f9d9d",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- To enable LoRA fine-tuning, set `RUN_LORA = True` in the LoRA cell.\n",
    "- LoRA section uses PEFT; ensure `peft`, `transformers`, `datasets`, and `torch` are installed.\n",
    "- Calibrated models use scikit-learn's `CalibratedClassifierCV` with both `sigmoid` and `isotonic` methods tested via OOF.\n",
    "- The ensemble weight is found by minimizing OOF log_loss over a simple 1D grid.\n",
    "- All submissions are written to the working directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
