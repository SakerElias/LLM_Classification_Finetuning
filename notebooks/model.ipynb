{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f023bdb",
   "metadata": {},
   "source": [
    "# ðŸš€ Modeling and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26045052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd, numpy as np, re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac6bb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data\n",
    "DATA = Path(\"../data\")\n",
    "TRAIN_PATH = DATA / 'train.csv'\n",
    "TEST_PATH = DATA / 'test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955efb89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15efa3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 3-class target: 0=A wins, 1=B wins, 2=Tie\n",
    "y = np.select(\n",
    "    [train_df['winner_model_a']==1, train_df['winner_model_b']==1, train_df['winner_tie']==1],\n",
    "    [0, 1, 2]\n",
    ")\n",
    "train_df['target'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c18088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for feature engineering\n",
    "\n",
    "# count regex matches safely\n",
    "def count_pattern(text, pattern):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(re.findall(pattern, text))\n",
    "\n",
    "# Paragraph breaks: \"\\n\\n\"\n",
    "def paragraph_count(text):\n",
    "    return text.count('\\\\n\\\\n')\n",
    "\n",
    "# List / bullet usage: \"-\", \"â€¢\", or numbered lists\n",
    "def list_count(text):\n",
    "    return count_pattern(text, r\"(^\\s*[\\-\\*â€¢]\\s|\\d+\\.)\")\n",
    "\n",
    "# Quote / markdown emphasis: \">\", \"**\", or blockquotes\n",
    "def quote_count(text):\n",
    "    return count_pattern(text, r\">|\\*\\*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f30a54",
   "metadata": {},
   "source": [
    "# This choice of features comes from our Exploratory Data Analysis in which we tested the correlation between prefered answers and different lexical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8946a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute structural features for A and B responses\n",
    "for df in [train_df, test_df]:\n",
    "    for col in ['response_a', 'response_b']:\n",
    "        df[f'para_count_{col[-1]}'] = df[col].apply(paragraph_count)\n",
    "        df[f'list_count_{col[-1]}'] = df[col].apply(list_count)\n",
    "        df[f'quote_count_{col[-1]}'] = df[col].apply(quote_count)\n",
    "\n",
    "    # Compute relative differences (A - B)\n",
    "    for feat in ['para_count', 'list_count', 'quote_count']:\n",
    "        df[f'{feat}_diff'] = df[f'{feat}_a'] - df[f'{feat}_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b856a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_diff</th>\n",
       "      <th>para_count_diff</th>\n",
       "      <th>list_count_diff</th>\n",
       "      <th>quote_count_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3332</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-535</td>\n",
       "      <td>-7</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-914</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1620</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len_diff  para_count_diff  list_count_diff  quote_count_diff\n",
       "0      3332               17                8                24\n",
       "1      -535               -7               -3                 0\n",
       "2      -914                1                0                 0\n",
       "3      1620                0                5                 0\n",
       "4       528               -2                0                 0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length features (characters)\n",
    "train_df['len_a'] = train_df['response_a'].astype(str).apply(len)\n",
    "train_df['len_b'] = train_df['response_b'].astype(str).apply(len)\n",
    "train_df['len_diff'] = train_df['len_a'] - train_df['len_b']\n",
    "\n",
    "# Compute metrics difference between a and b:\n",
    "train_df['para_count_diff']  = train_df['para_count_a']  - train_df['para_count_b']\n",
    "train_df['list_count_diff']  = train_df['list_count_a']  - train_df['list_count_b']\n",
    "train_df['quote_count_diff'] = train_df['quote_count_a'] - train_df['quote_count_b']\n",
    "\n",
    "# Final feature matrix\n",
    "feature_cols = ['len_diff', 'para_count_diff', 'list_count_diff', 'quote_count_diff']\n",
    "X = train_df[feature_cols].fillna(0)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "434df60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: log_loss = 1.07255\n",
      "Fold 2: log_loss = 1.07106\n",
      "Fold 3: log_loss = 1.07194\n",
      "Fold 4: log_loss = 1.07216\n",
      "Fold 5: log_loss = 1.06893\n",
      "\n",
      "Cross-val log_loss â†’ mean=1.07133, std=0.00130\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Define modeling pipeline\n",
    "pipe = Pipeline([\n",
    "    ('normalize', StandardScaler()),\n",
    "    ('model', LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        C=1.0,\n",
    "        class_weight=None\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Stratified 5-fold cross-validation setup\n",
    "cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold_scores = []\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y), start=1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict_proba(X_val)\n",
    "    loss = log_loss(y_val, preds, labels=[0, 1, 2])\n",
    "    fold_scores.append(loss)\n",
    "    print(f\"Fold {fold_idx}: log_loss = {loss:.5f}\")\n",
    "\n",
    "fold_scores = np.array(fold_scores)\n",
    "print(f\"\\nCross-val log_loss â†’ mean={fold_scores.mean():.5f}, std={fold_scores.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ae38b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_lr.csv\n"
     ]
    }
   ],
   "source": [
    "# Build test features\n",
    "test_df['len_a'] = test_df['response_a'].astype(str).apply(len)\n",
    "test_df['len_b'] = test_df['response_b'].astype(str).apply(len)\n",
    "test_df['len_diff'] = test_df['len_a'] - test_df['len_b']\n",
    "\n",
    "test_df['para_count_diff']  = test_df['para_count_a']  - test_df['para_count_b']\n",
    "test_df['list_count_diff']  = test_df['list_count_a']  - test_df['list_count_b']\n",
    "test_df['quote_count_diff'] = test_df['quote_count_a'] - test_df['quote_count_b']\n",
    "\n",
    "X_test = test_df[feature_cols].fillna(0)\n",
    "\n",
    "# Fit on full training data\n",
    "pipe.fit(X, y)\n",
    "\n",
    "# Predict probabilities for the 3 classes in Kaggle order:\n",
    "# winner_model_a (class 0), winner_model_b (class 1), winner_tie (class 2)\n",
    "proba_test = pipe.predict_proba(X_test)  # (n_test, 3)\n",
    "\n",
    "# Build submission\n",
    "sub = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': proba_test[:,0],\n",
    "    'winner_model_b': proba_test[:,1],\n",
    "    'winner_tie':     proba_test[:,2],\n",
    "})\n",
    "\n",
    "# Save\n",
    "sub.to_csv('submission_lr.csv', index=False)\n",
    "print(\"Saved submission_lr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433c386",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "693381f3",
   "metadata": {},
   "source": [
    "Text Preprocessing for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c39b79c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_text_from_field(text_field):\n",
    "    \"\"\"Extract text from string representation of list (e.g., '[\"text\"]' -> 'text').\"\"\"\n",
    "    try:\n",
    "        parsed = ast.literal_eval(text_field)\n",
    "        return ' '.join(parsed) if isinstance(parsed, list) else str(parsed)\n",
    "    except:\n",
    "        return str(text_field)\n",
    "\n",
    "# Apply to both train and test\n",
    "for df, df_name in [(train_df, 'train'), (test_df, 'test')]:\n",
    "    df['prompt_text'] = df['prompt'].apply(extract_text_from_field)\n",
    "    df['response_a_text'] = df['response_a'].apply(extract_text_from_field)\n",
    "    df['response_b_text'] = df['response_b'].apply(extract_text_from_field)\n",
    "    \n",
    "    # Combine prompt with responses (using [SEP] token)\n",
    "    df['text_a'] = df['prompt_text'] + \" [SEP] \" + df['response_a_text']\n",
    "    df['text_b'] = df['prompt_text'] + \" [SEP] \" + df['response_b_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fad258",
   "metadata": {},
   "source": [
    "Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf42e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168236fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding response A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1797/1797 [01:09<00:00, 25.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding response B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1797/1797 [01:16<00:00, 23.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training embeddings generated in 2.55 minutes\n",
      "   Shape A: (57477, 384)\n",
      "   Shape B: (57477, 384)\n"
     ]
    }
   ],
   "source": [
    "# Generate TRAINING embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nEmbedding response A...\")\n",
    "train_embeddings_a = model.encode(\n",
    "    train_df['text_a'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"\\nEmbedding response B...\")\n",
    "train_embeddings_b = model.encode(\n",
    "    train_df['text_b'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n Training embeddings generated in {elapsed/60:.2f} minutes\")\n",
    "print(f\"   Shape A: {train_embeddings_a.shape}\")\n",
    "print(f\"   Shape B: {train_embeddings_b.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('train_embeddings_a.npy', train_embeddings_a)\n",
    "np.save('train_embeddings_b.npy', train_embeddings_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06931322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding response A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding response B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Test embeddings generated in 0.00 minutes\n",
      "   Shape A: (3, 384)\n",
      "   Shape B: (3, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate TEST embeddings\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\nEmbedding response A...\")\n",
    "test_embeddings_a = model.encode(\n",
    "    test_df['text_a'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"\\nEmbedding response B...\")\n",
    "test_embeddings_b = model.encode(\n",
    "    test_df['text_b'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nâœ… Test embeddings generated in {elapsed/60:.2f} minutes\")\n",
    "print(f\"   Shape A: {test_embeddings_a.shape}\")\n",
    "print(f\"   Shape B: {test_embeddings_b.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('test_embeddings_a.npy', test_embeddings_a)\n",
    "np.save('test_embeddings_b.npy', test_embeddings_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3520c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embeddings for A and B\n",
    "X_train_embed = np.concatenate([train_embeddings_a, train_embeddings_b], axis=1)\n",
    "y_train = train_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695cf296",
   "metadata": {},
   "source": [
    "CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8741f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: log_loss = 1.07008\n",
      "Fold 2: log_loss = 1.07554\n",
      "Fold 3: log_loss = 1.06972\n",
      "Fold 4: log_loss = 1.07088\n",
      "Fold 5: log_loss = 1.07210\n",
      "\n",
      "Cross-val log_loss â†’ mean=1.07166, std=0.00210\n"
     ]
    }
   ],
   "source": [
    "cv_splitter = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X_train_embed, y_train), start=1):\n",
    "    X_train_fold = X_train_embed[train_idx]\n",
    "    X_val_fold = X_train_embed[val_idx]\n",
    "    y_train_fold = y_train[train_idx]\n",
    "    y_val_fold = y_train[val_idx]\n",
    "    \n",
    "    # Scale\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_fold)\n",
    "    X_val_scaled = scaler.transform(X_val_fold)\n",
    "    \n",
    "    # Train\n",
    "    model_lr = LogisticRegression(max_iter=2000, C=1.0, random_state=42)\n",
    "    model_lr.fit(X_train_scaled, y_train_fold)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred_proba = model_lr.predict_proba(X_val_scaled)\n",
    "    fold_loss = log_loss(y_val_fold, y_pred_proba)\n",
    "    cv_scores.append(fold_loss)\n",
    "    \n",
    "    print(f\"Fold {fold_idx}: log_loss = {fold_loss:.5f}\")\n",
    "\n",
    "print(f\"\\nCross-val log_loss â†’ mean={np.mean(cv_scores):.5f}, std={np.std(cv_scores):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679d14a",
   "metadata": {},
   "source": [
    "Embedding model (1.072) is performing worse than the lexical baseline (1.071). \n",
    "This implies lexical features capture important preference signals.\n",
    "The embedding approach might benefit from combining with lexical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "721057c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare test features\n",
    "X_test_embed = np.concatenate([test_embeddings_a, test_embeddings_b], axis=1)\n",
    "\n",
    "# Train on full dataset\n",
    "scaler_final = StandardScaler()\n",
    "X_train_scaled_full = scaler_final.fit_transform(X_train_embed)\n",
    "X_test_scaled = scaler_final.transform(X_test_embed)\n",
    "\n",
    "# Train final model\n",
    "final_model = LogisticRegression(max_iter=2000, C=1.0, random_state=42)\n",
    "final_model.fit(X_train_scaled_full, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "test_proba = final_model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Create submission\n",
    "submission_embed = pd.DataFrame({\n",
    "    'id': test_df['id'].values,\n",
    "    'winner_model_a': test_proba[:, 0],\n",
    "    'winner_model_b': test_proba[:, 1],\n",
    "    'winner_tie': test_proba[:, 2]\n",
    "})\n",
    "\n",
    "submission_embed.to_csv('submission_embeddings.csv', index=False)\n",
    "print(\"Saved submission_embeddings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
